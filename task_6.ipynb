{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>POLYGON ((-124.54615 40.86785, -124.54615 38.1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            geometry\n",
       "0  POLYGON ((-124.54615 40.86785, -124.54615 38.1..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "# Load the GeoJSON file\n",
    "geojson_path = 'train_data/challenge_6_bb_TRAIN.geojson'\n",
    "gdf = gpd.read_file(geojson_path)\n",
    "gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>POLYGON ((369704.958 4525237.457, 364581.501 4...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            geometry\n",
       "0  POLYGON ((369704.958 4525237.457, 364581.501 4..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyproj\n",
    "\n",
    "def get_utm_zone(longitude):\n",
    "    return int((longitude + 180) / 6) + 1\n",
    "\n",
    "# Get the bounds of the geometry\n",
    "minx, miny, maxx, maxy = gdf.geometry.bounds.iloc[0]\n",
    "\n",
    "# Calculate UTM zone\n",
    "utm_zone = get_utm_zone(minx)\n",
    "\n",
    "# Check for a suitable projection using pyproj\n",
    "proj = pyproj.Proj(proj='utm', zone=utm_zone, ellps='WGS84')\n",
    "\n",
    "# Get the corresponding EPSG code for the UTM zone using pyproj\n",
    "utm_crs = pyproj.CRS(f\"+proj=utm +zone={utm_zone} +datum=WGS84\")\n",
    "epsg_code = utm_crs.to_epsg()\n",
    "\n",
    "# Reproject the GeoDataFrame to the chosen EPSG code\n",
    "gdf = gdf.to_crs(epsg=epsg_code)\n",
    "gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>POINT (364581.501 4226949.476)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>POINT (367141.501 4226949.476)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>POINT (369701.501 4226949.476)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>POINT (372261.501 4226949.476)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>POINT (374821.501 4226949.476)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14620</th>\n",
       "      <td>POINT (671781.501 4523909.476)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14621</th>\n",
       "      <td>POINT (674341.501 4523909.476)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14622</th>\n",
       "      <td>POINT (676901.501 4523909.476)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14623</th>\n",
       "      <td>POINT (679461.501 4523909.476)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14624</th>\n",
       "      <td>POINT (682021.501 4523909.476)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14625 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             geometry\n",
       "0      POINT (364581.501 4226949.476)\n",
       "1      POINT (367141.501 4226949.476)\n",
       "2      POINT (369701.501 4226949.476)\n",
       "3      POINT (372261.501 4226949.476)\n",
       "4      POINT (374821.501 4226949.476)\n",
       "...                               ...\n",
       "14620  POINT (671781.501 4523909.476)\n",
       "14621  POINT (674341.501 4523909.476)\n",
       "14622  POINT (676901.501 4523909.476)\n",
       "14623  POINT (679461.501 4523909.476)\n",
       "14624  POINT (682021.501 4523909.476)\n",
       "\n",
       "[14625 rows x 1 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create a grid of points 5120m apart\n",
    "x = np.arange(gdf.total_bounds[0], gdf.total_bounds[2], 2560)\n",
    "y = np.arange(gdf.total_bounds[1], gdf.total_bounds[3], 2560)\n",
    "xx, yy = np.meshgrid(x, y)\n",
    "points = np.vstack([xx.ravel(), yy.ravel()]).T\n",
    "\n",
    "grid = gpd.GeoDataFrame(geometry=gpd.points_from_xy(points[:, 0], points[:, 1], crs=gdf.crs))\n",
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14625/14625 [8:02:38<00:00,  1.98s/it]  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>embeddings_2019</th>\n",
       "      <th>embeddings_2020</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-124.546147</td>\n",
       "      <td>38.180299</td>\n",
       "      <td>[[0.1155745, -0.016954176, 0.0016706283, -0.00...</td>\n",
       "      <td>[[0.1273245, -0.0030486751, 0.0007928024, 0.01...</td>\n",
       "      <td>POINT (-124.54615 38.18030)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-124.516928</td>\n",
       "      <td>38.180680</td>\n",
       "      <td>[[0.11867357, -0.017182661, 0.0028632905, -0.0...</td>\n",
       "      <td>[[0.12737001, -0.0030908359, 0.000729561, 0.01...</td>\n",
       "      <td>POINT (-124.51693 38.18068)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-124.487708</td>\n",
       "      <td>38.181054</td>\n",
       "      <td>[[0.11585856, -0.016491126, 0.005964425, -0.00...</td>\n",
       "      <td>[[0.12743236, -0.003174662, 0.00089427165, 0.0...</td>\n",
       "      <td>POINT (-124.48771 38.18105)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-124.458488</td>\n",
       "      <td>38.181421</td>\n",
       "      <td>[[0.11883765, -0.01688553, 0.0024151083, -0.00...</td>\n",
       "      <td>[[0.12739776, -0.0031370702, 0.0007280128, 0.0...</td>\n",
       "      <td>POINT (-124.45849 38.18142)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-124.429267</td>\n",
       "      <td>38.181780</td>\n",
       "      <td>[[0.1158724, -0.016283177, 0.0042494186, -0.00...</td>\n",
       "      <td>[[0.12756799, -0.0031793532, 0.00082945044, 0....</td>\n",
       "      <td>POINT (-124.42927 38.18178)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          lon        lat                                    embeddings_2019  \\\n",
       "0 -124.546147  38.180299  [[0.1155745, -0.016954176, 0.0016706283, -0.00...   \n",
       "1 -124.516928  38.180680  [[0.11867357, -0.017182661, 0.0028632905, -0.0...   \n",
       "2 -124.487708  38.181054  [[0.11585856, -0.016491126, 0.005964425, -0.00...   \n",
       "3 -124.458488  38.181421  [[0.11883765, -0.01688553, 0.0024151083, -0.00...   \n",
       "4 -124.429267  38.181780  [[0.1158724, -0.016283177, 0.0042494186, -0.00...   \n",
       "\n",
       "                                     embeddings_2020  \\\n",
       "0  [[0.1273245, -0.0030486751, 0.0007928024, 0.01...   \n",
       "1  [[0.12737001, -0.0030908359, 0.000729561, 0.01...   \n",
       "2  [[0.12743236, -0.003174662, 0.00089427165, 0.0...   \n",
       "3  [[0.12739776, -0.0031370702, 0.0007280128, 0.0...   \n",
       "4  [[0.12756799, -0.0031793532, 0.00082945044, 0....   \n",
       "\n",
       "                      geometry  \n",
       "0  POINT (-124.54615 38.18030)  \n",
       "1  POINT (-124.51693 38.18068)  \n",
       "2  POINT (-124.48771 38.18105)  \n",
       "3  POINT (-124.45849 38.18142)  \n",
       "4  POINT (-124.42927 38.18178)  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import pystac_client\n",
    "import stackstac\n",
    "import torch\n",
    "from torchvision import transforms as v2\n",
    "from box import Box\n",
    "import yaml\n",
    "import math\n",
    "from rasterio.enums import Resampling\n",
    "from tqdm import tqdm\n",
    "import rasterio\n",
    "import warnings\n",
    "import os\n",
    "import numpy as np\n",
    "import rioxarray  # Make sure to import rioxarray to extend xarray\n",
    "\n",
    "from src.model import ClayMAEModule\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "STAC_API = \"https://earth-search.aws.element84.com/v1\"\n",
    "COLLECTION = \"sentinel-2-l2a\"\n",
    "\n",
    "# Load the model and metadata\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ckpt = \"https://clay-model-ckpt.s3.amazonaws.com/v0.5.7/mae_v0.5.7_epoch-13_val-loss-0.3098.ckpt\"\n",
    "torch.set_default_device(device)\n",
    "\n",
    "torch.cuda.empty_cache()  # Clear GPU cache\n",
    "\n",
    "# Assuming grid is a GeoDataFrame with the points\n",
    "points = grid.to_crs(\"EPSG:4326\").geometry.apply(lambda x: (x.x, x.y)).tolist()\n",
    "\n",
    "model = ClayMAEModule.load_from_checkpoint(\n",
    "    ckpt, metadata_path=\"configs/metadata.yaml\", shuffle=False, mask_ratio=0\n",
    ")\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "\n",
    "metadata = Box(yaml.safe_load(open(\"configs/metadata.yaml\")))\n",
    "\n",
    "# Function to normalize timestamp\n",
    "def normalize_timestamp(date):\n",
    "    week = date.isocalendar().week * 2 * np.pi / 52\n",
    "    hour = date.hour * 2 * np.pi / 24\n",
    "    return (math.sin(week), math.cos(week)), (math.sin(hour), math.cos(hour))\n",
    "\n",
    "# Function to normalize lat/lon\n",
    "def normalize_latlon(lat, lon):\n",
    "    lat = lat * np.pi / 180\n",
    "    lon = lon * np.pi / 180\n",
    "    return (math.sin(lat), math.cos(lat)), (math.sin(lon), math.cos(lon))\n",
    "\n",
    "def to_device(data, device):\n",
    "    if isinstance(data, torch.Tensor):\n",
    "        return data.to(device)\n",
    "    elif isinstance(data, dict):\n",
    "        return {k: to_device(v, device) for k, v in data.items()}\n",
    "    elif isinstance(data, list):\n",
    "        return [to_device(v, device) for v in data]\n",
    "    return data\n",
    "\n",
    "def process_point(lon, lat, model, metadata, year, device, j):\n",
    "    model.to(device)  # Ensure the model is on the correct device\n",
    "    catalog = pystac_client.Client.open(STAC_API)\n",
    "    search = catalog.search(\n",
    "        collections=[COLLECTION],\n",
    "        datetime=f\"{year}-01-01/{year}-12-31\",\n",
    "        bbox=(lon - 1e-5, lat - 1e-5, lon + 1e-5, lat + 1e-5),\n",
    "        max_items=10,\n",
    "        query={\"eo:cloud_cover\": {\"lt\": 80}},\n",
    "    )\n",
    "\n",
    "    all_items = search.get_all_items()\n",
    "    items = list(all_items)\n",
    "    if not items:\n",
    "        return None\n",
    "    \n",
    "    items = sorted(items, key=lambda x: x.properties.get('eo:cloud_cover', float('inf')))\n",
    "    lowest_cloud_item = items[0]\n",
    "\n",
    "    epsg = lowest_cloud_item.properties[\"proj:epsg\"]\n",
    "\n",
    "    poidf = gpd.GeoDataFrame(\n",
    "        pd.DataFrame(),\n",
    "        crs=\"EPSG:4326\",\n",
    "        geometry=[Point(lon, lat)],\n",
    "    ).to_crs(epsg)\n",
    "\n",
    "    coords = poidf.iloc[0].geometry.coords[0]\n",
    "\n",
    "    size = 256\n",
    "    gsd = 10\n",
    "    bounds = (\n",
    "        coords[0] - (size * gsd) // 2,\n",
    "        coords[1] - (size * gsd) // 2,\n",
    "        coords[0] + (size * gsd) // 2,\n",
    "        coords[1] + (size * gsd) // 2,\n",
    "    )\n",
    "\n",
    "    stack = stackstac.stack(\n",
    "        lowest_cloud_item,\n",
    "        bounds=bounds,\n",
    "        snap_bounds=False,\n",
    "        epsg=epsg,\n",
    "        resolution=gsd,\n",
    "        dtype=\"float32\",\n",
    "        rescale=False,\n",
    "        fill_value=0,\n",
    "        assets=[\"blue\", \"green\", \"red\", \"nir\"],\n",
    "        resampling=Resampling.nearest,\n",
    "    )\n",
    "\n",
    "    stack = stack.compute()\n",
    "\n",
    "    items = []\n",
    "    dates = []\n",
    "    for item in all_items:\n",
    "        if item.datetime.date() not in dates:\n",
    "            items.append(item)\n",
    "            dates.append(item.datetime.date())\n",
    "\n",
    "    platform = \"sentinel-2-l2a\"\n",
    "    mean = []\n",
    "    std = []\n",
    "    waves = []\n",
    "    for band in stack.band:\n",
    "        mean.append(metadata[platform].bands.mean[str(band.values)])\n",
    "        std.append(metadata[platform].bands.std[str(band.values)])\n",
    "        waves.append(metadata[platform].bands.wavelength[str(band.values)])\n",
    "\n",
    "    transform = v2.Compose([v2.Normalize(mean=mean, std=std)])\n",
    "\n",
    "    datetimes = stack.time.values.astype(\"datetime64[s]\").tolist()\n",
    "    times = [normalize_timestamp(dat) for dat in datetimes]\n",
    "    week_norm = [dat[0] for dat in times]\n",
    "    hour_norm = [dat[1] for dat in times]\n",
    "\n",
    "    latlons = [normalize_latlon(lat, lon)] * len(times)\n",
    "    lat_norm = [dat[0] for dat in latlons]\n",
    "    lon_norm = [dat[1] for dat in latlons]\n",
    "\n",
    "    pixels = torch.from_numpy(stack.data.astype(np.float32)).to(device)\n",
    "    pixels = transform(pixels)\n",
    "\n",
    "    batch_size = 16\n",
    "    num_batches = math.ceil(len(stack) / batch_size)\n",
    "    \n",
    "    embeddings_list = []\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, len(stack))\n",
    "        \n",
    "        batch_pixels = pixels[start_idx:end_idx].to(device)\n",
    "        batch_time = torch.tensor(np.hstack((week_norm, hour_norm))[start_idx:end_idx], dtype=torch.float32).to(device)\n",
    "        batch_latlon = torch.tensor(np.hstack((lat_norm, lon_norm))[start_idx:end_idx], dtype=torch.float32).to(device)\n",
    "        \n",
    "        batch_datacube = {\n",
    "            \"platform\": platform,\n",
    "            \"time\": batch_time,\n",
    "            \"latlon\": batch_latlon,\n",
    "            \"pixels\": batch_pixels,\n",
    "            \"gsd\": torch.tensor(stack.gsd.values).to(device),\n",
    "            \"waves\": torch.tensor(waves).to(device),\n",
    "        }\n",
    "\n",
    "        batch_datacube = to_device(batch_datacube, device)\n",
    "\n",
    "        try:\n",
    "            model = model.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                unmsk_patch, _, _, _ = model.model.encoder(batch_datacube)\n",
    "            batch_embeddings = unmsk_patch[:, 0, :].cpu().numpy()\n",
    "            embeddings_list.append(batch_embeddings)\n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e):\n",
    "                print(f\"GPU OOM for point ({lon}, {lat}), batch {i+1}/{num_batches}. Trying CPU...\")\n",
    "                device = torch.device(\"cpu\")\n",
    "                batch_datacube = to_device(batch_datacube, device)\n",
    "                model = model.to(device)\n",
    "                with torch.no_grad():\n",
    "                    unmsk_patch, _, _, _ = model.model.encoder(batch_datacube)\n",
    "                batch_embeddings = unmsk_patch[:, 0, :].numpy()\n",
    "                embeddings_list.append(batch_embeddings)\n",
    "                device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "    embeddings = np.concatenate(embeddings_list, axis=0)\n",
    "    return embeddings\n",
    "\n",
    "# Initialize an empty dictionary to store results for both years\n",
    "results_dict = {\"lon\": [], \"lat\": [], \"embeddings_2019\": [], \"embeddings_2020\": []}\n",
    "\n",
    "# Specify the years for the datetime range in the search\n",
    "years = [2019, 2020]\n",
    "\n",
    "# Iterate through the points and process each one for both years\n",
    "for i, point in enumerate(tqdm(points)):\n",
    "    lon, lat = point\n",
    "    results_dict[\"lon\"].append(lon)\n",
    "    results_dict[\"lat\"].append(lat)\n",
    "    \n",
    "    for year in years:\n",
    "        embeddings = process_point(lon, lat, model, metadata, year, device, i)\n",
    "        if embeddings is not None:\n",
    "            results_dict[f\"embeddings_{year}\"].append(embeddings)\n",
    "        else:\n",
    "            results_dict[f\"embeddings_{year}\"].append(None)\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "df = pd.DataFrame(results_dict)\n",
    "\n",
    "# Convert to a GeoDataFrame\n",
    "gdf_results = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.lon, df.lat))\n",
    "\n",
    "# Output the resulting GeoDataFrame\n",
    "gdf_results.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_copy = gdf_results.copy()\n",
    "gdf_copy[\"embeddings_2019\"] = [embedding.flatten() if embedding is not None and embedding.size > 0 else None for embedding in gdf_results[\"embeddings_2019\"]]\n",
    "gdf_copy[\"embeddings_2020\"] = [embedding.flatten() if embedding is not None and embedding.size > 0 else None for embedding in gdf_results[\"embeddings_2020\"]]\n",
    "\n",
    "gdf_copy.to_parquet(\"train_data/challenge_6.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.read_parquet(\"train_data/challenge_6.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing geometries: 100%|██████████| 14625/14625 [03:15<00:00, 74.82it/s]\n"
     ]
    }
   ],
   "source": [
    "import ee\n",
    "import geopandas as gpd\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import os\n",
    "from shapely.geometry import box, mapping\n",
    "from shapely.ops import transform\n",
    "import pyproj\n",
    "\n",
    "# Initialize Earth Engine\n",
    "ee.Initialize()\n",
    "\n",
    "# Load the USFS/GTAC/MTBS annual burn severity mosaics dataset\n",
    "burn_severity = ee.ImageCollection('USFS/GTAC/MTBS/annual_burn_severity_mosaics/v1').filterDate('2020-01-01', '2020-12-31').mosaic()\n",
    "\n",
    "def create_bbox_around_point(point, size=2560):\n",
    "    # Create a bounding box around a point with the given size (meters)\n",
    "    half_size = size / 2.0\n",
    "    \n",
    "    # Define the projections\n",
    "    wgs84 = pyproj.CRS('EPSG:4326')\n",
    "    utm_zone = pyproj.CRS(epsg_code)  # Replace with appropriate UTM zone based on your location\n",
    "\n",
    "    # Project the point to UTM\n",
    "    project_to_utm = pyproj.Transformer.from_crs(wgs84, utm_zone, always_xy=True).transform\n",
    "    point_utm = transform(project_to_utm, point)\n",
    "\n",
    "    # Create the bounding box in UTM\n",
    "    bbox_utm = box(point_utm.x - half_size, point_utm.y - half_size, point_utm.x + half_size, point_utm.y + half_size)\n",
    "\n",
    "    # Project the bounding box back to WGS84\n",
    "    project_to_wgs84 = pyproj.Transformer.from_crs(utm_zone, wgs84, always_xy=True).transform\n",
    "    bbox_wgs84 = transform(project_to_wgs84, bbox_utm)\n",
    "    \n",
    "    return bbox_wgs84\n",
    "\n",
    "\n",
    "# def create_bbox_around_point(point, size=2560):\n",
    "#     # Create a bounding box around a point with the given size (meters)\n",
    "#     half_size = size / 2.0\n",
    "#     bbox = box(point.x - half_size, point.y - half_size, point.x + half_size, point.y + half_size)\n",
    "    \n",
    "#     # Define the projections\n",
    "#     wgs84 = pyproj.CRS('EPSG:4326')\n",
    "#     utm = pyproj.CRS(epsg_code)  # Use the appropriate UTM zone for your data\n",
    "\n",
    "#     project_to_wgs84 = pyproj.Transformer.from_crs(utm, wgs84, always_xy=True).transform\n",
    "\n",
    "#     # Create the bounding box in UTM\n",
    "#     expanded_bbox = box(\n",
    "#         bbox.bounds[0] - half_size, bbox.bounds[1] - half_size,\n",
    "#         bbox.bounds[2] + half_size, bbox.bounds[3] + half_size\n",
    "#     )\n",
    "\n",
    "#     # Project the bounding box back to WGS84\n",
    "#     bbox_wgs84 = transform(project_to_wgs84, expanded_bbox)\n",
    "    \n",
    "#     return bbox_wgs84\n",
    "\n",
    "def get_severity(geometry):\n",
    "    if geometry.geom_type == 'Point':\n",
    "        # Create a bounding box around the point\n",
    "        bbox = create_bbox_around_point(geometry)\n",
    "    else:\n",
    "        bbox = geometry\n",
    "    \n",
    "    # Convert the GeoPandas geometry to an Earth Engine geometry\n",
    "    ee_geometry = ee.Geometry(mapping(bbox))\n",
    "\n",
    "    # Get the mean severity value within the geometry\n",
    "    severity_value = burn_severity.reduceRegion(\n",
    "        reducer=ee.Reducer.mean(),\n",
    "        geometry=ee_geometry,\n",
    "        scale=30,\n",
    "        maxPixels=1e9\n",
    "    ).get('Severity')  # Adjust the band name if necessary\n",
    "    \n",
    "    # Return the result\n",
    "    return severity_value.getInfo() if severity_value is not None else None\n",
    "\n",
    "def process_geometries(combined_gdf):\n",
    "    # Get the total number of rows in the GeoDataFrame\n",
    "    total_rows = len(combined_gdf)\n",
    "\n",
    "    # Determine the number of threads to use\n",
    "    max_threads = 10  # Adjust this based on your system and Earth Engine quota\n",
    "    num_threads = min(total_rows, max_threads)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        # Submit all tasks\n",
    "        future_to_index = {executor.submit(get_severity, row.geometry): index \n",
    "                        for index, row in combined_gdf.iterrows()}\n",
    "        \n",
    "        # Process as they complete with a progress bar\n",
    "        with tqdm(total=total_rows, desc=\"Processing geometries\") as pbar:\n",
    "            for future in as_completed(future_to_index):\n",
    "                index = future_to_index[future]\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                except Exception as exc:\n",
    "                    print(f'Generated an exception: {exc}')\n",
    "                    result = None\n",
    "                results.append((index, result))\n",
    "                pbar.update(1)\n",
    "\n",
    "    # Sort results by index and extract only the values\n",
    "    sorted_results = [r[1] for r in sorted(results, key=lambda x: x[0])]\n",
    "    \n",
    "    return sorted_results\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Process the geometries\n",
    "    results = process_geometries(gdf)\n",
    "\n",
    "    # Add the results as a new column to the GeoDataFrame\n",
    "    gdf['mean_severity'] = results\n",
    "\n",
    "    # Save as Parquet file\n",
    "    gdf[['geometry', 'mean_severity', 'embeddings_2019', 'embeddings_2020']].to_parquet(\"train_data/challenge_6.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf['mean_severity'] = gdf['mean_severity'].fillna(0)\n",
    "gdf['embeddings_delta'] = gdf['embeddings_2020'] - gdf['embeddings_2019']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1723214395.824347   26101 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "[I 2024-08-09 14:39:55,830] A new study created in memory with name: no-name-9e3ce90c-52da-4d37-aa59-89427335b8af\n",
      "I0000 00:00:1723214395.827125   26101 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1723214395.828550   26101 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1723214395.838471   26101 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1723214395.839963   26101 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1723214395.841390   26101 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1723214395.848571   26101 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1723214395.850151   26101 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1723214395.851561   26101 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-09 14:39:55.852951: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13990 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1723214396.869066   30931 service.cc:146] XLA service 0x7f73dc003ec0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1723214396.869107   30931 service.cc:154]   StreamExecutor device (0): NVIDIA A10G, Compute Capability 8.6\n",
      "2024-08-09 14:39:56.896130: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-08-09 14:39:56.985108: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m162/293\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 939us/step - loss: 1.9309 - mae: 0.9004"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1723214398.013950   30931 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 15ms/step - loss: 1.5356 - mae: 0.7853 - val_loss: 0.2175 - val_mae: 0.2504\n",
      "Epoch 2/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2898 - mae: 0.3251 - val_loss: 0.1583 - val_mae: 0.1761\n",
      "Epoch 3/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1824 - mae: 0.2409 - val_loss: 0.1598 - val_mae: 0.1664\n",
      "Epoch 4/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1529 - mae: 0.2027 - val_loss: 0.1879 - val_mae: 0.1593\n",
      "Epoch 5/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1549 - mae: 0.1889 - val_loss: 0.1391 - val_mae: 0.1386\n",
      "Epoch 6/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1354 - mae: 0.1721 - val_loss: 0.1899 - val_mae: 0.1651\n",
      "Epoch 7/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1460 - mae: 0.1720 - val_loss: 0.1448 - val_mae: 0.1388\n",
      "Epoch 8/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1143 - mae: 0.1506 - val_loss: 0.1331 - val_mae: 0.1294\n",
      "Epoch 9/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1132 - mae: 0.1470 - val_loss: 0.1437 - val_mae: 0.1291\n",
      "Epoch 10/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1231 - mae: 0.1441 - val_loss: 0.1183 - val_mae: 0.1148\n",
      "Epoch 11/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1237 - mae: 0.1422 - val_loss: 0.1358 - val_mae: 0.1333\n",
      "Epoch 12/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1144 - mae: 0.1373 - val_loss: 0.1175 - val_mae: 0.1254\n",
      "Epoch 13/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1033 - mae: 0.1281 - val_loss: 0.1328 - val_mae: 0.1313\n",
      "Epoch 14/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1071 - mae: 0.1316 - val_loss: 0.1486 - val_mae: 0.1313\n",
      "Epoch 15/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1098 - mae: 0.1264 - val_loss: 0.1061 - val_mae: 0.1063\n",
      "Epoch 16/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1179 - mae: 0.1316 - val_loss: 0.1323 - val_mae: 0.1278\n",
      "Epoch 17/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1097 - mae: 0.1220 - val_loss: 0.1318 - val_mae: 0.1253\n",
      "Epoch 18/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1182 - mae: 0.1336 - val_loss: 0.1823 - val_mae: 0.1588\n",
      "Epoch 19/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1115 - mae: 0.1297 - val_loss: 0.1720 - val_mae: 0.1365\n",
      "Epoch 20/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0997 - mae: 0.1176 - val_loss: 0.0982 - val_mae: 0.1099\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-09 14:40:11,332] Trial 0 finished with value: 0.3553612408345228 and parameters: {'n_layers': 2, 'n_units_l0': 272, 'n_units_l1': 360, 'dropout_rate': 0.43104565373724735, 'learning_rate': 0.00096074398667019}. Best is trial 0 with value: 0.3553612408345228.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - loss: 8.6150 - mae: 0.9547 - val_loss: 0.2493 - val_mae: 0.2151\n",
      "Epoch 2/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2339 - mae: 0.2199 - val_loss: 0.1564 - val_mae: 0.1795\n",
      "Epoch 3/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1928 - mae: 0.1876 - val_loss: 0.1488 - val_mae: 0.1862\n",
      "Epoch 4/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1825 - mae: 0.1852 - val_loss: 0.1653 - val_mae: 0.1632\n",
      "Epoch 5/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1839 - mae: 0.1800 - val_loss: 0.4237 - val_mae: 0.2490\n",
      "Epoch 6/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2507 - mae: 0.2114 - val_loss: 0.3138 - val_mae: 0.2084\n",
      "Epoch 7/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2811 - mae: 0.2270 - val_loss: 0.2656 - val_mae: 0.2216\n",
      "Epoch 8/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2439 - mae: 0.2261 - val_loss: 0.2277 - val_mae: 0.2397\n",
      "Epoch 9/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2439 - mae: 0.2292 - val_loss: 0.3630 - val_mae: 0.2447\n",
      "Epoch 10/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4282 - mae: 0.2792 - val_loss: 0.4769 - val_mae: 0.3802\n",
      "Epoch 11/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6223 - mae: 0.3837 - val_loss: 0.6585 - val_mae: 0.4582\n",
      "Epoch 12/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6425 - mae: 0.4689 - val_loss: 0.6585 - val_mae: 0.4584\n",
      "Epoch 13/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.5594 - mae: 0.4215 - val_loss: 0.6576 - val_mae: 0.4697\n",
      "Epoch 14/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.5945 - mae: 0.4440 - val_loss: 0.6720 - val_mae: 0.3928\n",
      "Epoch 15/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6026 - mae: 0.4270 - val_loss: 0.6623 - val_mae: 0.4315\n",
      "Epoch 16/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6297 - mae: 0.4578 - val_loss: 0.6573 - val_mae: 0.4751\n",
      "Epoch 17/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6029 - mae: 0.4557 - val_loss: 0.7136 - val_mae: 0.4731\n",
      "Epoch 18/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6426 - mae: 0.4550 - val_loss: 0.4348 - val_mae: 0.3796\n",
      "Epoch 19/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.5827 - mae: 0.4109 - val_loss: 0.6571 - val_mae: 0.4799\n",
      "Epoch 20/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.5664 - mae: 0.4290 - val_loss: 0.6579 - val_mae: 0.4652\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-09 14:40:25,655] Trial 1 finished with value: 0.825847716483011 and parameters: {'n_layers': 2, 'n_units_l0': 243, 'n_units_l1': 350, 'dropout_rate': 0.21831573997296683, 'learning_rate': 0.009020270673316445}. Best is trial 0 with value: 0.3553612408345228.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 2.3180 - mae: 0.9660 - val_loss: 0.2761 - val_mae: 0.3271\n",
      "Epoch 2/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3937 - mae: 0.4221 - val_loss: 0.1739 - val_mae: 0.2462\n",
      "Epoch 3/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1981 - mae: 0.2883 - val_loss: 0.1478 - val_mae: 0.2149\n",
      "Epoch 4/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1483 - mae: 0.2398 - val_loss: 0.1301 - val_mae: 0.2001\n",
      "Epoch 5/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1435 - mae: 0.2220 - val_loss: 0.1292 - val_mae: 0.2055\n",
      "Epoch 6/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1338 - mae: 0.2122 - val_loss: 0.1242 - val_mae: 0.1863\n",
      "Epoch 7/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1273 - mae: 0.2023 - val_loss: 0.1161 - val_mae: 0.1791\n",
      "Epoch 8/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1302 - mae: 0.2044 - val_loss: 0.1232 - val_mae: 0.1748\n",
      "Epoch 9/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1219 - mae: 0.1934 - val_loss: 0.1226 - val_mae: 0.1810\n",
      "Epoch 10/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1176 - mae: 0.1873 - val_loss: 0.1236 - val_mae: 0.1863\n",
      "Epoch 11/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1143 - mae: 0.1854 - val_loss: 0.1250 - val_mae: 0.1780\n",
      "Epoch 12/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1081 - mae: 0.1838 - val_loss: 0.1278 - val_mae: 0.1836\n",
      "Epoch 13/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1218 - mae: 0.1972 - val_loss: 0.1336 - val_mae: 0.1985\n",
      "Epoch 14/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1194 - mae: 0.1921 - val_loss: 0.1155 - val_mae: 0.1810\n",
      "Epoch 15/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1044 - mae: 0.1776 - val_loss: 0.1195 - val_mae: 0.1882\n",
      "Epoch 16/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1157 - mae: 0.1916 - val_loss: 0.1296 - val_mae: 0.1996\n",
      "Epoch 17/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1132 - mae: 0.1896 - val_loss: 0.1315 - val_mae: 0.1873\n",
      "Epoch 18/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1202 - mae: 0.1896 - val_loss: 0.1232 - val_mae: 0.2057\n",
      "Epoch 19/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1101 - mae: 0.1871 - val_loss: 0.1376 - val_mae: 0.2101\n",
      "Epoch 20/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1153 - mae: 0.1877 - val_loss: 0.1494 - val_mae: 0.2100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-09 14:40:35,457] Trial 2 finished with value: 0.40885032133663946 and parameters: {'n_layers': 1, 'n_units_l0': 178, 'dropout_rate': 0.41237914883175186, 'learning_rate': 0.0012502864203174304}. Best is trial 0 with value: 0.3553612408345228.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - loss: 6.6570 - mae: 0.9055 - val_loss: 0.3542 - val_mae: 0.2861\n",
      "Epoch 2/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4769 - mae: 0.3276 - val_loss: 0.6578 - val_mae: 0.4981\n",
      "Epoch 3/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6306 - mae: 0.4473 - val_loss: 0.6573 - val_mae: 0.4746\n",
      "Epoch 4/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6554 - mae: 0.4619 - val_loss: 0.6525 - val_mae: 0.4379\n",
      "Epoch 5/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6571 - mae: 0.4407 - val_loss: 0.6582 - val_mae: 0.4618\n",
      "Epoch 6/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.5874 - mae: 0.4338 - val_loss: 0.6576 - val_mae: 0.4688\n",
      "Epoch 7/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6207 - mae: 0.4596 - val_loss: 0.6588 - val_mae: 0.4556\n",
      "Epoch 8/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6167 - mae: 0.4413 - val_loss: 0.6583 - val_mae: 0.4597\n",
      "Epoch 9/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.5757 - mae: 0.4309 - val_loss: 0.6570 - val_mae: 0.4926\n",
      "Epoch 10/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6043 - mae: 0.4472 - val_loss: 0.6590 - val_mae: 0.4536\n",
      "Epoch 11/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.5854 - mae: 0.4303 - val_loss: 0.6571 - val_mae: 0.4803\n",
      "Epoch 12/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.5972 - mae: 0.4375 - val_loss: 0.6610 - val_mae: 0.4389\n",
      "Epoch 13/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6035 - mae: 0.4402 - val_loss: 0.6597 - val_mae: 0.4481\n",
      "Epoch 14/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6179 - mae: 0.4478 - val_loss: 0.6671 - val_mae: 0.4099\n",
      "Epoch 15/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.5901 - mae: 0.4300 - val_loss: 0.6584 - val_mae: 0.4591\n",
      "Epoch 16/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.5776 - mae: 0.4272 - val_loss: 0.6570 - val_mae: 0.4906\n",
      "Epoch 17/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.5998 - mae: 0.4548 - val_loss: 0.6580 - val_mae: 0.4640\n",
      "Epoch 18/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6568 - mae: 0.4599 - val_loss: 0.6596 - val_mae: 0.4485\n",
      "Epoch 19/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.5943 - mae: 0.4404 - val_loss: 0.6570 - val_mae: 0.4874\n",
      "Epoch 20/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.5957 - mae: 0.4473 - val_loss: 0.6570 - val_mae: 0.4923\n",
      "\u001b[1m73/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 699us/step  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-09 14:40:52.588165: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_18', 40 bytes spill stores, 40 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-09 14:40:53,471] Trial 3 finished with value: 0.8248667771971087 and parameters: {'n_layers': 3, 'n_units_l0': 294, 'n_units_l1': 248, 'n_units_l2': 380, 'dropout_rate': 0.38461843556139164, 'learning_rate': 0.00867389510916457}. Best is trial 0 with value: 0.3553612408345228.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 4.3271 - mae: 1.1284 - val_loss: 0.2056 - val_mae: 0.2822\n",
      "Epoch 2/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2592 - mae: 0.3385 - val_loss: 0.1586 - val_mae: 0.2493\n",
      "Epoch 3/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1621 - mae: 0.2574 - val_loss: 0.1324 - val_mae: 0.2079\n",
      "Epoch 4/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1236 - mae: 0.2143 - val_loss: 0.1254 - val_mae: 0.1907\n",
      "Epoch 5/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1216 - mae: 0.2092 - val_loss: 0.1468 - val_mae: 0.2398\n",
      "Epoch 6/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1175 - mae: 0.2043 - val_loss: 0.1274 - val_mae: 0.1973\n",
      "Epoch 7/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1084 - mae: 0.1943 - val_loss: 0.1287 - val_mae: 0.2043\n",
      "Epoch 8/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1203 - mae: 0.2021 - val_loss: 0.1185 - val_mae: 0.1866\n",
      "Epoch 9/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1037 - mae: 0.1907 - val_loss: 0.1504 - val_mae: 0.2620\n",
      "Epoch 10/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1237 - mae: 0.2134 - val_loss: 0.1155 - val_mae: 0.1929\n",
      "Epoch 11/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1134 - mae: 0.2021 - val_loss: 0.1363 - val_mae: 0.2186\n",
      "Epoch 12/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1117 - mae: 0.2013 - val_loss: 0.1357 - val_mae: 0.2245\n",
      "Epoch 13/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1146 - mae: 0.2080 - val_loss: 0.1562 - val_mae: 0.2391\n",
      "Epoch 14/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1207 - mae: 0.2100 - val_loss: 0.1240 - val_mae: 0.2183\n",
      "Epoch 15/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1392 - mae: 0.2303 - val_loss: 0.1458 - val_mae: 0.2292\n",
      "Epoch 16/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1249 - mae: 0.2127 - val_loss: 0.1295 - val_mae: 0.2079\n",
      "Epoch 17/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1364 - mae: 0.2234 - val_loss: 0.1511 - val_mae: 0.2353\n",
      "Epoch 18/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1275 - mae: 0.2172 - val_loss: 0.1591 - val_mae: 0.2481\n",
      "Epoch 19/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1380 - mae: 0.2244 - val_loss: 0.1790 - val_mae: 0.2427\n",
      "Epoch 20/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1408 - mae: 0.2314 - val_loss: 0.1407 - val_mae: 0.2028\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-09 14:41:03,341] Trial 4 finished with value: 0.39372504782110795 and parameters: {'n_layers': 1, 'n_units_l0': 484, 'dropout_rate': 0.23582210463105585, 'learning_rate': 0.001756710464128349}. Best is trial 0 with value: 0.3553612408345228.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_layers': 2, 'n_units_l0': 272, 'n_units_l1': 360, 'dropout_rate': 0.43104565373724735, 'learning_rate': 0.00096074398667019}\n",
      "Epoch 1/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 1.3326 - mae: 0.7342 - val_loss: 0.2772 - val_mae: 0.2391\n",
      "Epoch 2/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2552 - mae: 0.3003 - val_loss: 0.1689 - val_mae: 0.1890\n",
      "Epoch 3/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1904 - mae: 0.2341 - val_loss: 0.1410 - val_mae: 0.1550\n",
      "Epoch 4/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1690 - mae: 0.2102 - val_loss: 0.1208 - val_mae: 0.1393\n",
      "Epoch 5/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1478 - mae: 0.1879 - val_loss: 0.1627 - val_mae: 0.1433\n",
      "Epoch 6/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1456 - mae: 0.1795 - val_loss: 0.1330 - val_mae: 0.1272\n",
      "Epoch 7/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1215 - mae: 0.1509 - val_loss: 0.1567 - val_mae: 0.1464\n",
      "Epoch 8/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1261 - mae: 0.1527 - val_loss: 0.1346 - val_mae: 0.1270\n",
      "Epoch 9/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1237 - mae: 0.1458 - val_loss: 0.1512 - val_mae: 0.1348\n",
      "Epoch 10/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1147 - mae: 0.1412 - val_loss: 0.1120 - val_mae: 0.1114\n",
      "Epoch 11/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1180 - mae: 0.1385 - val_loss: 0.1107 - val_mae: 0.1151\n",
      "Epoch 12/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1110 - mae: 0.1351 - val_loss: 0.1175 - val_mae: 0.1184\n",
      "Epoch 13/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1011 - mae: 0.1234 - val_loss: 0.1320 - val_mae: 0.1305\n",
      "Epoch 14/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1010 - mae: 0.1228 - val_loss: 0.1680 - val_mae: 0.1453\n",
      "Epoch 15/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1195 - mae: 0.1331 - val_loss: 0.1685 - val_mae: 0.1456\n",
      "Epoch 16/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1286 - mae: 0.1387 - val_loss: 0.1773 - val_mae: 0.1457\n",
      "Epoch 17/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1151 - mae: 0.1281 - val_loss: 0.1970 - val_mae: 0.1482\n",
      "Epoch 18/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1324 - mae: 0.1406 - val_loss: 0.1760 - val_mae: 0.1497\n",
      "Epoch 19/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1052 - mae: 0.1236 - val_loss: 0.1262 - val_mae: 0.1228\n",
      "Epoch 20/20\n",
      "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1043 - mae: 0.1182 - val_loss: 0.1398 - val_mae: 0.1225\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1762 - mae: 0.1384 \n",
      "Test MAE: 0.1317\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 0.4056\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['models/6_scaler.joblib']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Input\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_shape, layers, dropout_rate, learning_rate, device):\n",
    "        self.input_shape = input_shape\n",
    "        self.layers = layers\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.device = device\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Input(shape=(self.input_shape,)))\n",
    "        for layer_size in self.layers:\n",
    "            model.add(Dense(layer_size, activation='relu'))\n",
    "            model.add(Dropout(self.dropout_rate))\n",
    "        model.add(Dense(1, activation='linear'))  # Output layer for regression\n",
    "        return model\n",
    "\n",
    "    def compile_model(self):\n",
    "        optimizer = Adam(learning_rate=self.learning_rate)\n",
    "        self.model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mae'])  # Use MSE for regression\n",
    "\n",
    "    def train_model(self, X_train, y_train, epochs=20, batch_size=32, validation_split=0.2):\n",
    "        with tf.device(self.device):\n",
    "            history = self.model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=validation_split)\n",
    "        return history\n",
    "\n",
    "    def evaluate_model(self, X_test, y_test):\n",
    "        with tf.device(self.device):\n",
    "            loss, mae = self.model.evaluate(X_test, y_test)\n",
    "        return loss, mae\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        with tf.device(self.device):\n",
    "            predictions = self.model.predict(X_test)\n",
    "        return predictions.flatten()\n",
    "\n",
    "    def calculate_rmse(self, y_test, predictions):\n",
    "        return np.sqrt(mean_squared_error(y_test, predictions))\n",
    "    \n",
    "    def save_model(self, filename):\n",
    "        self.model.save(filename)\n",
    "\n",
    "    @classmethod\n",
    "    def load_model(cls, filename, input_shape, device):\n",
    "        loaded_model = tf.keras.models.load_model(filename)\n",
    "        nn = cls(input_shape, [], 0, 0, device)  # Dummy values for layers, dropout_rate, and learning_rate\n",
    "        nn.model = loaded_model\n",
    "        return nn\n",
    "\n",
    "def objective(trial):\n",
    "    layers = []\n",
    "    for i in range(trial.suggest_int('n_layers', 1, 3)):\n",
    "        layers.append(trial.suggest_int(f'n_units_l{i}', 64, 512))\n",
    "    \n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.2, 0.5)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2)\n",
    "    \n",
    "    nn = NeuralNetwork(input_shape=X_train.shape[1], device=device, layers=layers, dropout_rate=dropout_rate, learning_rate=learning_rate)\n",
    "    nn.compile_model()\n",
    "    \n",
    "    nn.train_model(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2)\n",
    "    \n",
    "    predictions = nn.predict(X_test)\n",
    "    rmse = nn.calculate_rmse(y_test, predictions)\n",
    "    \n",
    "    return rmse\n",
    "\n",
    "# Assuming combined_gdf is already loaded\n",
    "combined_gdf_filtered = gdf.dropna()\n",
    "\n",
    "# Flatten the nested arrays\n",
    "def flatten_embeddings(embedding):\n",
    "    if isinstance(embedding, np.ndarray) and embedding.ndim > 0:\n",
    "        return np.concatenate(embedding) if embedding.ndim > 1 else embedding.flatten()\n",
    "    else:\n",
    "        return np.array([])  # Handle cases where embedding might be zero-dimensional or empty\n",
    "\n",
    "X = np.array([flatten_embeddings(x) for x in combined_gdf_filtered['embeddings_delta'] if flatten_embeddings(x).size > 0])\n",
    "y = combined_gdf_filtered['mean_severity']\n",
    "\n",
    "# Ensure that X and y have matching lengths after filtering\n",
    "X = np.array([x for i, x in enumerate(X) if flatten_embeddings(combined_gdf_filtered['embeddings_delta'].iloc[i]).size > 0])\n",
    "y = y.iloc[:X.shape[0]]  # Adjust y to have the same number of samples as X\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "device = '/GPU:0' if tf.config.list_physical_devices('GPU') else '/CPU:0'\n",
    "\n",
    "# Optimize the hyperparameters\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=5)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(study.best_params)\n",
    "\n",
    "# Example usage with the best hyperparameters\n",
    "best_params = study.best_params\n",
    "layers = [best_params[f'n_units_l{i}'] for i in range(best_params['n_layers'])]\n",
    "dropout_rate = best_params['dropout_rate']\n",
    "learning_rate = best_params['learning_rate']\n",
    "\n",
    "nn = NeuralNetwork(input_shape=X_train.shape[1], device=device, layers=layers, dropout_rate=dropout_rate, learning_rate=learning_rate)\n",
    "nn.compile_model()\n",
    "history = nn.train_model(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2)\n",
    "loss, mae = nn.evaluate_model(X_test, y_test)\n",
    "print(f'Test MAE: {mae:.4f}')\n",
    "predictions = nn.predict(X_test)\n",
    "rmse = nn.calculate_rmse(y_test, predictions)\n",
    "print(f'Test RMSE: {rmse:.4f}')\n",
    "\n",
    "# Save the model\n",
    "nn.save_model('models/task_6_model.h5')\n",
    "\n",
    "# Save scaler\n",
    "joblib.dump(scaler, 'models/6_scaler.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>POLYGON ((-115.57766 40.70403, -115.57766 42.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            geometry\n",
       "0  POLYGON ((-115.57766 40.70403, -115.57766 42.0..."
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "year = 2022\n",
    "\n",
    "# Load the GeoJSON file\n",
    "geojson_path = 'test_data/challenge_6_bb.geojson'\n",
    "gdf = gpd.read_file(geojson_path)\n",
    "gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>POLYGON ((620156.362 4506875.086, 617625.674 4...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            geometry\n",
       "0  POLYGON ((620156.362 4506875.086, 617625.674 4..."
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyproj\n",
    "\n",
    "def get_utm_zone(longitude):\n",
    "    return int((longitude + 180) / 6) + 1\n",
    "\n",
    "# Get the bounds of the geometry\n",
    "minx, miny, maxx, maxy = gdf.geometry.bounds.iloc[0]\n",
    "\n",
    "# Calculate UTM zone\n",
    "utm_zone = get_utm_zone(minx)\n",
    "\n",
    "# Check for a suitable projection using pyproj\n",
    "proj = pyproj.Proj(proj='utm', zone=utm_zone, ellps='WGS84')\n",
    "\n",
    "# Get the corresponding EPSG code for the UTM zone using pyproj\n",
    "utm_crs = pyproj.CRS(f\"+proj=utm +zone={utm_zone} +datum=WGS84\")\n",
    "epsg_code = utm_crs.to_epsg()\n",
    "\n",
    "# Reproject the GeoDataFrame to the chosen EPSG code\n",
    "gdf = gdf.to_crs(epsg=epsg_code)\n",
    "gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>POINT (444829.369 4506107.413)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>POINT (447389.369 4506107.413)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>POINT (449949.369 4506107.413)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>POINT (452509.369 4506107.413)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>POINT (455069.369 4506107.413)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4204</th>\n",
       "      <td>POINT (608669.369 4659707.413)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4205</th>\n",
       "      <td>POINT (611229.369 4659707.413)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4206</th>\n",
       "      <td>POINT (613789.369 4659707.413)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4207</th>\n",
       "      <td>POINT (616349.369 4659707.413)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4208</th>\n",
       "      <td>POINT (618909.369 4659707.413)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4209 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            geometry\n",
       "0     POINT (444829.369 4506107.413)\n",
       "1     POINT (447389.369 4506107.413)\n",
       "2     POINT (449949.369 4506107.413)\n",
       "3     POINT (452509.369 4506107.413)\n",
       "4     POINT (455069.369 4506107.413)\n",
       "...                              ...\n",
       "4204  POINT (608669.369 4659707.413)\n",
       "4205  POINT (611229.369 4659707.413)\n",
       "4206  POINT (613789.369 4659707.413)\n",
       "4207  POINT (616349.369 4659707.413)\n",
       "4208  POINT (618909.369 4659707.413)\n",
       "\n",
       "[4209 rows x 1 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create a grid of points 5120m apart\n",
    "x = np.arange(gdf.total_bounds[0], gdf.total_bounds[2], 2560)\n",
    "y = np.arange(gdf.total_bounds[1], gdf.total_bounds[3], 2560)\n",
    "xx, yy = np.meshgrid(x, y)\n",
    "points = np.vstack([xx.ravel(), yy.ravel()]).T\n",
    "\n",
    "grid = gpd.GeoDataFrame(geometry=gpd.points_from_xy(points[:, 0], points[:, 1], crs=gdf.crs))\n",
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4209/4209 [2:28:18<00:00,  2.11s/it]  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>embeddings_2017</th>\n",
       "      <th>embeddings_2018</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-117.653086</td>\n",
       "      <td>40.704029</td>\n",
       "      <td>[[-0.093266055, -0.011566908, 0.034409165, 0.1...</td>\n",
       "      <td>[[-0.07646129, -0.047068905, -0.016069224, 0.1...</td>\n",
       "      <td>POINT (-117.65309 40.70403)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-117.622783</td>\n",
       "      <td>40.704197</td>\n",
       "      <td>[[-0.07906168, 0.0015968415, -0.034547463, 0.1...</td>\n",
       "      <td>[[-0.049535986, 0.06324335, -0.05352257, 0.183...</td>\n",
       "      <td>POINT (-117.62278 40.70420)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-117.592480</td>\n",
       "      <td>40.704356</td>\n",
       "      <td>[[0.051406052, -0.02938476, 0.11883815, 0.1303...</td>\n",
       "      <td>[[-0.06238199, 0.06399791, 0.0098732365, 0.160...</td>\n",
       "      <td>POINT (-117.59248 40.70436)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-117.562178</td>\n",
       "      <td>40.704508</td>\n",
       "      <td>[[-0.05635409, -0.039113358, -0.010543512, 0.0...</td>\n",
       "      <td>[[-0.057337593, 0.04036558, 0.036062606, 0.146...</td>\n",
       "      <td>POINT (-117.56218 40.70451)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-117.531874</td>\n",
       "      <td>40.704651</td>\n",
       "      <td>[[-0.06445381, -0.041924402, -0.04184392, 0.12...</td>\n",
       "      <td>[[-0.07720847, 0.021437781, -0.05698099, 0.148...</td>\n",
       "      <td>POINT (-117.53187 40.70465)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          lon        lat                                    embeddings_2017  \\\n",
       "0 -117.653086  40.704029  [[-0.093266055, -0.011566908, 0.034409165, 0.1...   \n",
       "1 -117.622783  40.704197  [[-0.07906168, 0.0015968415, -0.034547463, 0.1...   \n",
       "2 -117.592480  40.704356  [[0.051406052, -0.02938476, 0.11883815, 0.1303...   \n",
       "3 -117.562178  40.704508  [[-0.05635409, -0.039113358, -0.010543512, 0.0...   \n",
       "4 -117.531874  40.704651  [[-0.06445381, -0.041924402, -0.04184392, 0.12...   \n",
       "\n",
       "                                     embeddings_2018  \\\n",
       "0  [[-0.07646129, -0.047068905, -0.016069224, 0.1...   \n",
       "1  [[-0.049535986, 0.06324335, -0.05352257, 0.183...   \n",
       "2  [[-0.06238199, 0.06399791, 0.0098732365, 0.160...   \n",
       "3  [[-0.057337593, 0.04036558, 0.036062606, 0.146...   \n",
       "4  [[-0.07720847, 0.021437781, -0.05698099, 0.148...   \n",
       "\n",
       "                      geometry  \n",
       "0  POINT (-117.65309 40.70403)  \n",
       "1  POINT (-117.62278 40.70420)  \n",
       "2  POINT (-117.59248 40.70436)  \n",
       "3  POINT (-117.56218 40.70451)  \n",
       "4  POINT (-117.53187 40.70465)  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import pystac_client\n",
    "import stackstac\n",
    "import torch\n",
    "from torchvision import transforms as v2\n",
    "from box import Box\n",
    "import yaml\n",
    "import math\n",
    "from rasterio.enums import Resampling\n",
    "from tqdm import tqdm\n",
    "import rasterio\n",
    "import warnings\n",
    "import os\n",
    "import numpy as np\n",
    "import rioxarray  # Make sure to import rioxarray to extend xarray\n",
    "\n",
    "from src.model import ClayMAEModule\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "STAC_API = \"https://earth-search.aws.element84.com/v1\"\n",
    "COLLECTION = \"sentinel-2-l2a\"\n",
    "\n",
    "# Load the model and metadata\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ckpt = \"https://clay-model-ckpt.s3.amazonaws.com/v0.5.7/mae_v0.5.7_epoch-13_val-loss-0.3098.ckpt\"\n",
    "torch.set_default_device(device)\n",
    "\n",
    "torch.cuda.empty_cache()  # Clear GPU cache\n",
    "\n",
    "# Assuming grid is a GeoDataFrame with the points\n",
    "points = grid.to_crs(\"EPSG:4326\").geometry.apply(lambda x: (x.x, x.y)).tolist()\n",
    "\n",
    "model = ClayMAEModule.load_from_checkpoint(\n",
    "    ckpt, metadata_path=\"configs/metadata.yaml\", shuffle=False, mask_ratio=0\n",
    ")\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "\n",
    "metadata = Box(yaml.safe_load(open(\"configs/metadata.yaml\")))\n",
    "\n",
    "# Function to normalize timestamp\n",
    "def normalize_timestamp(date):\n",
    "    week = date.isocalendar().week * 2 * np.pi / 52\n",
    "    hour = date.hour * 2 * np.pi / 24\n",
    "    return (math.sin(week), math.cos(week)), (math.sin(hour), math.cos(hour))\n",
    "\n",
    "# Function to normalize lat/lon\n",
    "def normalize_latlon(lat, lon):\n",
    "    lat = lat * np.pi / 180\n",
    "    lon = lon * np.pi / 180\n",
    "    return (math.sin(lat), math.cos(lat)), (math.sin(lon), math.cos(lon))\n",
    "\n",
    "def to_device(data, device):\n",
    "    if isinstance(data, torch.Tensor):\n",
    "        return data.to(device)\n",
    "    elif isinstance(data, dict):\n",
    "        return {k: to_device(v, device) for k, v in data.items()}\n",
    "    elif isinstance(data, list):\n",
    "        return [to_device(v, device) for v in data]\n",
    "    return data\n",
    "\n",
    "def process_point(lon, lat, model, metadata, year, device, j):\n",
    "    model.to(device)  # Ensure the model is on the correct device\n",
    "    catalog = pystac_client.Client.open(STAC_API)\n",
    "    search = catalog.search(\n",
    "        collections=[COLLECTION],\n",
    "        datetime=f\"{year}-01-01/{year}-12-31\",\n",
    "        bbox=(lon - 1e-5, lat - 1e-5, lon + 1e-5, lat + 1e-5),\n",
    "        max_items=10,\n",
    "        query={\"eo:cloud_cover\": {\"lt\": 80}},\n",
    "    )\n",
    "\n",
    "    all_items = search.get_all_items()\n",
    "    items = list(all_items)\n",
    "    if not items:\n",
    "        return None\n",
    "    \n",
    "    items = sorted(items, key=lambda x: x.properties.get('eo:cloud_cover', float('inf')))\n",
    "    lowest_cloud_item = items[0]\n",
    "\n",
    "    epsg = lowest_cloud_item.properties[\"proj:epsg\"]\n",
    "\n",
    "    poidf = gpd.GeoDataFrame(\n",
    "        pd.DataFrame(),\n",
    "        crs=\"EPSG:4326\",\n",
    "        geometry=[Point(lon, lat)],\n",
    "    ).to_crs(epsg)\n",
    "\n",
    "    coords = poidf.iloc[0].geometry.coords[0]\n",
    "\n",
    "    size = 256\n",
    "    gsd = 10\n",
    "    bounds = (\n",
    "        coords[0] - (size * gsd) // 2,\n",
    "        coords[1] - (size * gsd) // 2,\n",
    "        coords[0] + (size * gsd) // 2,\n",
    "        coords[1] + (size * gsd) // 2,\n",
    "    )\n",
    "\n",
    "    stack = stackstac.stack(\n",
    "        lowest_cloud_item,\n",
    "        bounds=bounds,\n",
    "        snap_bounds=False,\n",
    "        epsg=epsg,\n",
    "        resolution=gsd,\n",
    "        dtype=\"float32\",\n",
    "        rescale=False,\n",
    "        fill_value=0,\n",
    "        assets=[\"blue\", \"green\", \"red\", \"nir\"],\n",
    "        resampling=Resampling.nearest,\n",
    "    )\n",
    "\n",
    "    stack = stack.compute()\n",
    "\n",
    "    items = []\n",
    "    dates = []\n",
    "    for item in all_items:\n",
    "        if item.datetime.date() not in dates:\n",
    "            items.append(item)\n",
    "            dates.append(item.datetime.date())\n",
    "\n",
    "    platform = \"sentinel-2-l2a\"\n",
    "    mean = []\n",
    "    std = []\n",
    "    waves = []\n",
    "    for band in stack.band:\n",
    "        mean.append(metadata[platform].bands.mean[str(band.values)])\n",
    "        std.append(metadata[platform].bands.std[str(band.values)])\n",
    "        waves.append(metadata[platform].bands.wavelength[str(band.values)])\n",
    "\n",
    "    transform = v2.Compose([v2.Normalize(mean=mean, std=std)])\n",
    "\n",
    "    datetimes = stack.time.values.astype(\"datetime64[s]\").tolist()\n",
    "    times = [normalize_timestamp(dat) for dat in datetimes]\n",
    "    week_norm = [dat[0] for dat in times]\n",
    "    hour_norm = [dat[1] for dat in times]\n",
    "\n",
    "    latlons = [normalize_latlon(lat, lon)] * len(times)\n",
    "    lat_norm = [dat[0] for dat in latlons]\n",
    "    lon_norm = [dat[1] for dat in latlons]\n",
    "\n",
    "    pixels = torch.from_numpy(stack.data.astype(np.float32)).to(device)\n",
    "    pixels = transform(pixels)\n",
    "\n",
    "    batch_size = 16\n",
    "    num_batches = math.ceil(len(stack) / batch_size)\n",
    "    \n",
    "    embeddings_list = []\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, len(stack))\n",
    "        \n",
    "        batch_pixels = pixels[start_idx:end_idx].to(device)\n",
    "        batch_time = torch.tensor(np.hstack((week_norm, hour_norm))[start_idx:end_idx], dtype=torch.float32).to(device)\n",
    "        batch_latlon = torch.tensor(np.hstack((lat_norm, lon_norm))[start_idx:end_idx], dtype=torch.float32).to(device)\n",
    "        \n",
    "        batch_datacube = {\n",
    "            \"platform\": platform,\n",
    "            \"time\": batch_time,\n",
    "            \"latlon\": batch_latlon,\n",
    "            \"pixels\": batch_pixels,\n",
    "            \"gsd\": torch.tensor(stack.gsd.values).to(device),\n",
    "            \"waves\": torch.tensor(waves).to(device),\n",
    "        }\n",
    "\n",
    "        batch_datacube = to_device(batch_datacube, device)\n",
    "\n",
    "        try:\n",
    "            model = model.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                unmsk_patch, _, _, _ = model.model.encoder(batch_datacube)\n",
    "            batch_embeddings = unmsk_patch[:, 0, :].cpu().numpy()\n",
    "            embeddings_list.append(batch_embeddings)\n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e):\n",
    "                print(f\"GPU OOM for point ({lon}, {lat}), batch {i+1}/{num_batches}. Trying CPU...\")\n",
    "                device = torch.device(\"cpu\")\n",
    "                batch_datacube = to_device(batch_datacube, device)\n",
    "                model = model.to(device)\n",
    "                with torch.no_grad():\n",
    "                    unmsk_patch, _, _, _ = model.model.encoder(batch_datacube)\n",
    "                batch_embeddings = unmsk_patch[:, 0, :].numpy()\n",
    "                embeddings_list.append(batch_embeddings)\n",
    "                device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "    embeddings = np.concatenate(embeddings_list, axis=0)\n",
    "    return embeddings\n",
    "\n",
    "# Initialize an empty dictionary to store results for both years\n",
    "results_dict = {\"lon\": [], \"lat\": [], \"embeddings_2017\": [], \"embeddings_2018\": []}\n",
    "\n",
    "# Specify the years for the datetime range in the search\n",
    "years = [2017, 2018]\n",
    "\n",
    "# Iterate through the points and process each one for both years\n",
    "for i, point in enumerate(tqdm(points)):\n",
    "    lon, lat = point\n",
    "    results_dict[\"lon\"].append(lon)\n",
    "    results_dict[\"lat\"].append(lat)\n",
    "    \n",
    "    for year in years:\n",
    "        embeddings = process_point(lon, lat, model, metadata, year, device, i)\n",
    "        if embeddings is not None:\n",
    "            results_dict[f\"embeddings_{year}\"].append(embeddings)\n",
    "        else:\n",
    "            results_dict[f\"embeddings_{year}\"].append(None)\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "df = pd.DataFrame(results_dict)\n",
    "\n",
    "# Convert to a GeoDataFrame\n",
    "gdf_results = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.lon, df.lat))\n",
    "\n",
    "# Output the resulting GeoDataFrame\n",
    "gdf_results.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_copy = gdf_results.copy()\n",
    "gdf_copy[\"embeddings_2017\"] = [embedding.flatten() if embedding is not None and embedding.size > 0 else None for embedding in gdf_results[\"embeddings_2017\"]]\n",
    "gdf_copy[\"embeddings_2018\"] = [embedding.flatten() if embedding is not None and embedding.size > 0 else None for embedding in gdf_results[\"embeddings_2018\"]]\n",
    "\n",
    "gdf_copy.to_parquet(\"test_data/challenge_6.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing geometries: 100%|██████████| 4209/4209 [00:54<00:00, 77.58it/s]\n"
     ]
    }
   ],
   "source": [
    "import ee\n",
    "import geopandas as gpd\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import os\n",
    "from shapely.geometry import box, mapping\n",
    "from shapely.ops import transform\n",
    "import pyproj\n",
    "\n",
    "# Initialize Earth Engine\n",
    "ee.Initialize()\n",
    "\n",
    "# Load the USFS/GTAC/MTBS annual burn severity mosaics dataset\n",
    "burn_severity = ee.ImageCollection('USFS/GTAC/MTBS/annual_burn_severity_mosaics/v1').filterDate('2018-01-01', '2018-12-31').mosaic()\n",
    "\n",
    "def create_bbox_around_point(point, size=2560):\n",
    "    # Create a bounding box around a point with the given size (meters)\n",
    "    half_size = size / 2.0\n",
    "    \n",
    "    # Define the projections\n",
    "    wgs84 = pyproj.CRS('EPSG:4326')\n",
    "    utm_zone = pyproj.CRS(epsg_code)  # Replace with appropriate UTM zone based on your location\n",
    "\n",
    "    # Project the point to UTM\n",
    "    project_to_utm = pyproj.Transformer.from_crs(wgs84, utm_zone, always_xy=True).transform\n",
    "    point_utm = transform(project_to_utm, point)\n",
    "\n",
    "    # Create the bounding box in UTM\n",
    "    bbox_utm = box(point_utm.x - half_size, point_utm.y - half_size, point_utm.x + half_size, point_utm.y + half_size)\n",
    "\n",
    "    # Project the bounding box back to WGS84\n",
    "    project_to_wgs84 = pyproj.Transformer.from_crs(utm_zone, wgs84, always_xy=True).transform\n",
    "    bbox_wgs84 = transform(project_to_wgs84, bbox_utm)\n",
    "    \n",
    "    return bbox_wgs84\n",
    "\n",
    "def get_severity(geometry):\n",
    "    if geometry.geom_type == 'Point':\n",
    "        # Create a bounding box around the point\n",
    "        bbox = create_bbox_around_point(geometry)\n",
    "    else:\n",
    "        bbox = geometry\n",
    "    \n",
    "    # Convert the GeoPandas geometry to an Earth Engine geometry\n",
    "    ee_geometry = ee.Geometry(mapping(bbox))\n",
    "\n",
    "    # Get the mean severity value within the geometry\n",
    "    severity_value = burn_severity.reduceRegion(\n",
    "        reducer=ee.Reducer.mean(),\n",
    "        geometry=ee_geometry,\n",
    "        scale=30,\n",
    "        maxPixels=1e9\n",
    "    ).get('Severity')  # Adjust the band name if necessary\n",
    "    \n",
    "    # Return the result\n",
    "    return severity_value.getInfo() if severity_value is not None else None\n",
    "\n",
    "def process_geometries(combined_gdf):\n",
    "    # Get the total number of rows in the GeoDataFrame\n",
    "    total_rows = len(combined_gdf)\n",
    "\n",
    "    # Determine the number of threads to use\n",
    "    max_threads = 10  # Adjust this based on your system and Earth Engine quota\n",
    "    num_threads = min(total_rows, max_threads)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        # Submit all tasks\n",
    "        future_to_index = {executor.submit(get_severity, row.geometry): index \n",
    "                        for index, row in combined_gdf.iterrows()}\n",
    "        \n",
    "        # Process as they complete with a progress bar\n",
    "        with tqdm(total=total_rows, desc=\"Processing geometries\") as pbar:\n",
    "            for future in as_completed(future_to_index):\n",
    "                index = future_to_index[future]\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                except Exception as exc:\n",
    "                    print(f'Generated an exception: {exc}')\n",
    "                    result = None\n",
    "                results.append((index, result))\n",
    "                pbar.update(1)\n",
    "\n",
    "    # Sort results by index and extract only the values\n",
    "    sorted_results = [r[1] for r in sorted(results, key=lambda x: x[0])]\n",
    "    \n",
    "    return sorted_results\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Process the geometries\n",
    "    results = process_geometries(gdf_copy)\n",
    "\n",
    "    # Add the results as a new column to the GeoDataFrame\n",
    "    gdf_copy['mean_severity'] = results\n",
    "\n",
    "    # Save as Parquet file\n",
    "    gdf_copy[['geometry', 'mean_severity', 'embeddings_2017', 'embeddings_2018']].to_parquet(\"test_data/challenge_6.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_copy['mean_severity'] = gdf_copy['mean_severity'].fillna(0)\n",
    "gdf_copy['embeddings_delta'] = gdf_copy['embeddings_2018'] - gdf_copy['embeddings_2017']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Detect if GPU is available\n",
    "device = '/GPU:0' if tf.config.list_physical_devices('GPU') else '/CPU:0'\n",
    "\n",
    "# Load the model\n",
    "loaded_nn = NeuralNetwork.load_model('models/agb_regression_model.h5', input_shape=768, device=device)\n",
    "\n",
    "# Prepare your new data (assuming it's in the same format as your training data)\n",
    "new_data = np.squeeze(gdf_copy['embeddings_delta'].tolist())\n",
    "new_data = pd.DataFrame(new_data)  # Ensure the new data is in DataFrame format\n",
    "\n",
    "# Standardize the new data using the saved scaler\n",
    "scaler = joblib.load('models/scaler.joblib')\n",
    "new_data_scaled = scaler.transform(new_data)\n",
    "\n",
    "# Make predictions\n",
    "new_predictions = loaded_nn.predict(new_data_scaled)\n",
    "\n",
    "gdf_copy['pred_severity'] = new_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set RMSE: 1.842589182391211\n",
      "Test set corr: 0.05585754431984979\n"
     ]
    }
   ],
   "source": [
    "print(\"Test set RMSE:\", np.sqrt(np.mean((gdf_copy['pred_severity']-gdf_copy['mean_severity'])**2)))\n",
    "print(\"Test set corr:\", np.corrcoef(gdf_copy['pred_severity'],gdf_copy['mean_severity'])[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_copy[['geometry', 'mean_severity', 'pred_severity']].to_file(\"~/severity.geojson\", driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "claymodel-latest-v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
