{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>POLYGON ((-90.23406 29.05768, -89.28282 29.242...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            geometry\n",
       "0  POLYGON ((-90.23406 29.05768, -89.28282 29.242..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "# Load the GeoJSON file\n",
    "geojson_path = 'train_data/challenge_5_bb_TRAIN.geojson'\n",
    "gdf = gpd.read_file(geojson_path)\n",
    "gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>POLYGON ((769308.177 3217535.091, 861338.486 3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            geometry\n",
       "0  POLYGON ((769308.177 3217535.091, 861338.486 3..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyproj\n",
    "\n",
    "def get_utm_zone(longitude):\n",
    "    return int((longitude + 180) / 6) + 1\n",
    "\n",
    "# Get the bounds of the geometry\n",
    "minx, miny, maxx, maxy = gdf.geometry.bounds.iloc[0]\n",
    "\n",
    "# Calculate UTM zone\n",
    "utm_zone = get_utm_zone(minx)\n",
    "\n",
    "# Check for a suitable projection using pyproj\n",
    "proj = pyproj.Proj(proj='utm', zone=utm_zone, ellps='WGS84')\n",
    "\n",
    "# Get the corresponding EPSG code for the UTM zone using pyproj\n",
    "utm_crs = pyproj.CRS(f\"+proj=utm +zone={utm_zone} +datum=WGS84\")\n",
    "epsg_code = utm_crs.to_epsg()\n",
    "\n",
    "# Reproject the GeoDataFrame to the chosen EPSG code\n",
    "gdf = gdf.to_crs(epsg=epsg_code)\n",
    "gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# # Create a grid of points 5120m apart\n",
    "# x = np.arange(gdf.total_bounds[0], gdf.total_bounds[2], 2560)\n",
    "# y = np.arange(gdf.total_bounds[1], gdf.total_bounds[3], 2560)\n",
    "# xx, yy = np.meshgrid(x, y)\n",
    "# points = np.vstack([xx.ravel(), yy.ravel()]).T\n",
    "\n",
    "# grid = gpd.GeoDataFrame(geometry=gpd.points_from_xy(points[:, 0], points[:, 1], crs=gdf.crs))\n",
    "# grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import geopandas as gpd\n",
    "# from shapely.geometry import Point\n",
    "# import pystac_client\n",
    "# import stackstac\n",
    "# import torch\n",
    "# from torchvision import transforms as v2\n",
    "# from box import Box\n",
    "# import yaml\n",
    "# import math\n",
    "# from rasterio.enums import Resampling\n",
    "# from tqdm import tqdm\n",
    "# import rasterio\n",
    "# import warnings\n",
    "# import os\n",
    "# import numpy as np\n",
    "# import rioxarray  # Make sure to import rioxarray to extend xarray\n",
    "\n",
    "# from src.model import ClayMAEModule\n",
    "\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# STAC_API = \"https://earth-search.aws.element84.com/v1\"\n",
    "# COLLECTION = \"sentinel-2-l2a\"\n",
    "\n",
    "# # Load the model and metadata\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# ckpt = \"https://clay-model-ckpt.s3.amazonaws.com/v0.5.7/mae_v0.5.7_epoch-13_val-loss-0.3098.ckpt\"\n",
    "# torch.set_default_device(device)\n",
    "\n",
    "# torch.cuda.empty_cache()  # Clear GPU cache\n",
    "\n",
    "# # Assuming grid is a GeoDataFrame with the points\n",
    "# points = grid.to_crs(\"EPSG:4326\").geometry.apply(lambda x: (x.x, x.y)).tolist()\n",
    "\n",
    "# model = ClayMAEModule.load_from_checkpoint(\n",
    "#     ckpt, metadata_path=\"configs/metadata.yaml\", shuffle=False, mask_ratio=0\n",
    "# )\n",
    "# model.eval()\n",
    "# model = model.to(device)\n",
    "\n",
    "# metadata = Box(yaml.safe_load(open(\"configs/metadata.yaml\")))\n",
    "\n",
    "# # Function to normalize timestamp\n",
    "# def normalize_timestamp(date):\n",
    "#     week = date.isocalendar().week * 2 * np.pi / 52\n",
    "#     hour = date.hour * 2 * np.pi / 24\n",
    "#     return (math.sin(week), math.cos(week)), (math.sin(hour), math.cos(hour))\n",
    "\n",
    "# # Function to normalize lat/lon\n",
    "# def normalize_latlon(lat, lon):\n",
    "#     lat = lat * np.pi / 180\n",
    "#     lon = lon * np.pi / 180\n",
    "#     return (math.sin(lat), math.cos(lat)), (math.sin(lon), math.cos(lon))\n",
    "\n",
    "# def to_device(data, device):\n",
    "#     if isinstance(data, torch.Tensor):\n",
    "#         return data.to(device)\n",
    "#     elif isinstance(data, dict):\n",
    "#         return {k: to_device(v, device) for k, v in data.items()}\n",
    "#     elif isinstance(data, list):\n",
    "#         return [to_device(v, device) for v in data]\n",
    "#     return data\n",
    "\n",
    "# def process_point(lon, lat, model, metadata, year, device, j):\n",
    "#     model.to(device)  # Ensure the model is on the correct device\n",
    "#     catalog = pystac_client.Client.open(STAC_API)\n",
    "#     search = catalog.search(\n",
    "#         collections=[COLLECTION],\n",
    "#         datetime=f\"{year}-08-01/{year}-09-30\",\n",
    "#         bbox=(lon - 1e-5, lat - 1e-5, lon + 1e-5, lat + 1e-5),\n",
    "#         max_items=10,\n",
    "#         query={\"eo:cloud_cover\": {\"lt\": 80}},\n",
    "#     )\n",
    "\n",
    "#     all_items = search.get_all_items()\n",
    "#     items = list(all_items)\n",
    "#     if not items:\n",
    "#         return None\n",
    "    \n",
    "#     items = sorted(items, key=lambda x: x.properties.get('eo:cloud_cover', float('inf')))\n",
    "#     lowest_cloud_item = items[0]\n",
    "\n",
    "#     epsg = lowest_cloud_item.properties[\"proj:epsg\"]\n",
    "\n",
    "#     poidf = gpd.GeoDataFrame(\n",
    "#         pd.DataFrame(),\n",
    "#         crs=\"EPSG:4326\",\n",
    "#         geometry=[Point(lon, lat)],\n",
    "#     ).to_crs(epsg)\n",
    "\n",
    "#     coords = poidf.iloc[0].geometry.coords[0]\n",
    "\n",
    "#     size = 256\n",
    "#     gsd = 10\n",
    "#     bounds = (\n",
    "#         coords[0] - (size * gsd) // 2,\n",
    "#         coords[1] - (size * gsd) // 2,\n",
    "#         coords[0] + (size * gsd) // 2,\n",
    "#         coords[1] + (size * gsd) // 2,\n",
    "#     )\n",
    "\n",
    "#     stack = stackstac.stack(\n",
    "#         lowest_cloud_item,\n",
    "#         bounds=bounds,\n",
    "#         snap_bounds=False,\n",
    "#         epsg=epsg,\n",
    "#         resolution=gsd,\n",
    "#         dtype=\"float32\",\n",
    "#         rescale=False,\n",
    "#         fill_value=0,\n",
    "#         assets=[\"blue\", \"green\", \"red\", \"nir\"],\n",
    "#         resampling=Resampling.nearest,\n",
    "#     )\n",
    "\n",
    "#     stack = stack.compute()\n",
    "\n",
    "#     items = []\n",
    "#     dates = []\n",
    "#     for item in all_items:\n",
    "#         if item.datetime.date() not in dates:\n",
    "#             items.append(item)\n",
    "#             dates.append(item.datetime.date())\n",
    "\n",
    "#     platform = \"sentinel-2-l2a\"\n",
    "#     mean = []\n",
    "#     std = []\n",
    "#     waves = []\n",
    "#     for band in stack.band:\n",
    "#         mean.append(metadata[platform].bands.mean[str(band.values)])\n",
    "#         std.append(metadata[platform].bands.std[str(band.values)])\n",
    "#         waves.append(metadata[platform].bands.wavelength[str(band.values)])\n",
    "\n",
    "#     transform = v2.Compose([v2.Normalize(mean=mean, std=std)])\n",
    "\n",
    "#     datetimes = stack.time.values.astype(\"datetime64[s]\").tolist()\n",
    "#     times = [normalize_timestamp(dat) for dat in datetimes]\n",
    "#     week_norm = [dat[0] for dat in times]\n",
    "#     hour_norm = [dat[1] for dat in times]\n",
    "\n",
    "#     latlons = [normalize_latlon(lat, lon)] * len(times)\n",
    "#     lat_norm = [dat[0] for dat in latlons]\n",
    "#     lon_norm = [dat[1] for dat in latlons]\n",
    "\n",
    "#     pixels = torch.from_numpy(stack.data.astype(np.float32)).to(device)\n",
    "#     pixels = transform(pixels)\n",
    "\n",
    "#     batch_size = 16\n",
    "#     num_batches = math.ceil(len(stack) / batch_size)\n",
    "    \n",
    "#     embeddings_list = []\n",
    "#     for i in range(num_batches):\n",
    "#         start_idx = i * batch_size\n",
    "#         end_idx = min((i + 1) * batch_size, len(stack))\n",
    "        \n",
    "#         batch_pixels = pixels[start_idx:end_idx].to(device)\n",
    "#         batch_time = torch.tensor(np.hstack((week_norm, hour_norm))[start_idx:end_idx], dtype=torch.float32).to(device)\n",
    "#         batch_latlon = torch.tensor(np.hstack((lat_norm, lon_norm))[start_idx:end_idx], dtype=torch.float32).to(device)\n",
    "        \n",
    "#         batch_datacube = {\n",
    "#             \"platform\": platform,\n",
    "#             \"time\": batch_time,\n",
    "#             \"latlon\": batch_latlon,\n",
    "#             \"pixels\": batch_pixels,\n",
    "#             \"gsd\": torch.tensor(stack.gsd.values).to(device),\n",
    "#             \"waves\": torch.tensor(waves).to(device),\n",
    "#         }\n",
    "\n",
    "#         batch_datacube = to_device(batch_datacube, device)\n",
    "\n",
    "#         try:\n",
    "#             model = model.to(device)\n",
    "\n",
    "#             with torch.no_grad():\n",
    "#                 unmsk_patch, _, _, _ = model.model.encoder(batch_datacube)\n",
    "#             batch_embeddings = unmsk_patch[:, 0, :].cpu().numpy()\n",
    "#             embeddings_list.append(batch_embeddings)\n",
    "#         except RuntimeError as e:\n",
    "#             if \"out of memory\" in str(e):\n",
    "#                 print(f\"GPU OOM for point ({lon}, {lat}), batch {i+1}/{num_batches}. Trying CPU...\")\n",
    "#                 device = torch.device(\"cpu\")\n",
    "#                 batch_datacube = to_device(batch_datacube, device)\n",
    "#                 model = model.to(device)\n",
    "#                 with torch.no_grad():\n",
    "#                     unmsk_patch, _, _, _ = model.model.encoder(batch_datacube)\n",
    "#                 batch_embeddings = unmsk_patch[:, 0, :].numpy()\n",
    "#                 embeddings_list.append(batch_embeddings)\n",
    "#                 device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#             else:\n",
    "#                 raise e\n",
    "\n",
    "#     embeddings = np.concatenate(embeddings_list, axis=0)\n",
    "#     return embeddings\n",
    "\n",
    "# # Specify the years for the datetime range in the search\n",
    "# years = [2018, 2019]\n",
    "\n",
    "# # Initialize an empty dictionary to store results for both years\n",
    "# results_dict = {\"lon\": [], \"lat\": [], f\"embeddings_{years[0]}\": [], f\"embeddings_{years[1]}\": []}\n",
    "\n",
    "# # Iterate through the points and process each one for both years\n",
    "# for i, point in enumerate(tqdm(points)):\n",
    "#     lon, lat = point\n",
    "#     results_dict[\"lon\"].append(lon)\n",
    "#     results_dict[\"lat\"].append(lat)\n",
    "    \n",
    "#     for year in years:\n",
    "#         embeddings = process_point(lon, lat, model, metadata, year, device, i)\n",
    "#         if embeddings is not None:\n",
    "#             results_dict[f\"embeddings_{year}\"].append(embeddings)\n",
    "#         else:\n",
    "#             results_dict[f\"embeddings_{year}\"].append(None)\n",
    "\n",
    "# # Create a DataFrame from the results\n",
    "# df = pd.DataFrame(results_dict)\n",
    "\n",
    "# # Convert to a GeoDataFrame\n",
    "# gdf_results = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.lon, df.lat))\n",
    "\n",
    "# # Output the resulting GeoDataFrame\n",
    "# gdf_results.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gdf_copy = gdf_results.copy()\n",
    "# gdf_copy[\"embeddings_2018\"] = [embedding.flatten() if embedding is not None and embedding.size > 0 else None for embedding in gdf_results[\"embeddings_2018\"]]\n",
    "# gdf_copy[\"embeddings_2019\"] = [embedding.flatten() if embedding is not None and embedding.size > 0 else None for embedding in gdf_results[\"embeddings_2019\"]]\n",
    "\n",
    "# gdf_copy.to_parquet(\"train_data/challenge_5.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.read_parquet(\"train_data/challenge_5.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Join flood data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Raster 1: 100%|██████████| 6764/6764 [46:57<00:00,  2.40it/s]\n",
      "Processing Raster 2: 100%|██████████| 6764/6764 [43:06<00:00,  2.62it/s]\n"
     ]
    }
   ],
   "source": [
    "# import geopandas as gpd\n",
    "# import rasterio\n",
    "# import xarray as xr\n",
    "# import numpy as np\n",
    "# import rioxarray\n",
    "# from tqdm import tqdm\n",
    "# from joblib import Parallel, delayed\n",
    "\n",
    "# # Load your GeoDataFrame\n",
    "# # gdf = gpd.read_file('your_geopandas_dataframe.gpkg')\n",
    "\n",
    "# # Load the TIFF files as xarray Datasets\n",
    "# raster1_path = 'train_data/flood_WM_S1A_IW_GRDH_1SDV_20190816T000150_20190816T000215_028587_033BC6_D6AA.tif'\n",
    "# raster2_path = 'train_data/flood_WM_S1A_IW_GRDH_1SDV_20190816T000215_20190816T000240_028587_033BC6_4F87.tif'\n",
    "\n",
    "# # Convert the TIFFs to xarray datasets\n",
    "# raster1_xr = rioxarray.open_rasterio(raster1_path)\n",
    "# raster2_xr = rioxarray.open_rasterio(raster2_path)\n",
    "\n",
    "# def get_raster_value_at_geometry_xr(geometry, raster_xr):\n",
    "#     try:\n",
    "#         clipped = raster_xr.rio.clip([geometry], raster_xr.rio.crs)\n",
    "#         return float(clipped.mean().values)\n",
    "#     except rioxarray.exceptions.NoDataInBounds:\n",
    "#         return np.nan\n",
    "\n",
    "# def process_row(geom, raster_xr):\n",
    "#     return get_raster_value_at_geometry_xr(geom, raster_xr)\n",
    "\n",
    "# # Parallel processing with joblib\n",
    "# n_jobs = 12 # Use all available cores\n",
    "\n",
    "# # Use tqdm with joblib for progress bar\n",
    "# raster_values_1 = Parallel(n_jobs=n_jobs)(\n",
    "#     delayed(process_row)(geom, raster1_xr) for geom in tqdm(gdf['geometry'], desc=\"Processing Raster 1\"))\n",
    "\n",
    "# raster_values_2 = Parallel(n_jobs=n_jobs)(\n",
    "#     delayed(process_row)(geom, raster2_xr) for geom in tqdm(gdf['geometry'], desc=\"Processing Raster 2\"))\n",
    "\n",
    "# # Assign the values back to the GeoDataFrame\n",
    "# gdf['raster_value_1'] = raster_values_1\n",
    "# gdf['raster_value_2'] = raster_values_2\n",
    "\n",
    "# # Set values greater than 1 to NaN for raster_value_1\n",
    "# gdf.loc[gdf['raster_value_1'] > 1, 'raster_value_1'] = np.nan\n",
    "\n",
    "# # Set values greater than 1 to NaN for raster_value_2\n",
    "# gdf.loc[gdf['raster_value_2'] > 1, 'raster_value_2'] = np.nan\n",
    "\n",
    "# gdf['flood'] = gdf[['raster_value_1', 'raster_value_2']].max(axis=1)\n",
    "\n",
    "# gdf.to_parquet(\"train_data/challenge_5.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf['embeddings_delta'] = gdf['embeddings_2019'] - gdf['embeddings_2018']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1723645330.195529    9023 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "[I 2024-08-14 14:22:10,703] A new study created in memory with name: no-name-ec57d292-8111-4341-aa20-dc94a0650e65\n",
      "I0000 00:00:1723645330.695229    9023 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1723645330.699063    9023 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1723645330.717183    9023 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1723645330.720866    9023 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1723645330.724373    9023 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1723645330.903919    9023 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1723645330.905425    9023 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1723645330.906855    9023 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-14 14:22:10.909515: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20723 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1723645332.324256   10259 service.cc:146] XLA service 0x7fc36800b920 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1723645332.324296   10259 service.cc:154]   StreamExecutor device (0): NVIDIA A10G, Compute Capability 8.6\n",
      "2024-08-14 14:22:12.434582: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-08-14 14:22:12.812378: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m47/88\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7521 - loss: 0.5496 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1723645334.730835   10259 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.7767 - loss: 0.4975"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-14 14:22:19.208224: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_28', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "2024-08-14 14:22:21.524396: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_28', 84 bytes spill stores, 84 bytes spill loads\n",
      "\n",
      "2024-08-14 14:22:21.743777: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_35', 72 bytes spill stores, 72 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 97ms/step - accuracy: 0.7771 - loss: 0.4967 - val_accuracy: 0.8734 - val_loss: 0.2825\n",
      "Epoch 2/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8728 - loss: 0.2921 - val_accuracy: 0.8791 - val_loss: 0.2651\n",
      "Epoch 3/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8751 - loss: 0.2996 - val_accuracy: 0.8933 - val_loss: 0.2527\n",
      "Epoch 4/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8810 - loss: 0.2585 - val_accuracy: 0.8962 - val_loss: 0.2527\n",
      "Epoch 5/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9032 - loss: 0.2351 - val_accuracy: 0.9104 - val_loss: 0.2674\n",
      "Epoch 6/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9108 - loss: 0.2267 - val_accuracy: 0.9004 - val_loss: 0.2502\n",
      "Epoch 7/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9227 - loss: 0.1940 - val_accuracy: 0.8962 - val_loss: 0.2606\n",
      "Epoch 8/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9280 - loss: 0.1660 - val_accuracy: 0.8890 - val_loss: 0.2678\n",
      "Epoch 9/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9183 - loss: 0.1762 - val_accuracy: 0.8976 - val_loss: 0.2515\n",
      "Epoch 10/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9357 - loss: 0.1521 - val_accuracy: 0.9033 - val_loss: 0.2485\n",
      "Epoch 11/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9314 - loss: 0.1478 - val_accuracy: 0.8962 - val_loss: 0.2776\n",
      "Epoch 12/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9476 - loss: 0.1358 - val_accuracy: 0.8919 - val_loss: 0.2974\n",
      "Epoch 13/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9360 - loss: 0.1554 - val_accuracy: 0.9061 - val_loss: 0.2900\n",
      "Epoch 14/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9535 - loss: 0.1232 - val_accuracy: 0.9018 - val_loss: 0.3070\n",
      "Epoch 15/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9518 - loss: 0.1151 - val_accuracy: 0.8933 - val_loss: 0.3263\n",
      "Epoch 16/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9525 - loss: 0.1307 - val_accuracy: 0.8990 - val_loss: 0.3124\n",
      "Epoch 17/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9563 - loss: 0.0995 - val_accuracy: 0.8890 - val_loss: 0.3104\n",
      "Epoch 18/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9584 - loss: 0.1037 - val_accuracy: 0.8905 - val_loss: 0.3425\n",
      "Epoch 19/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9565 - loss: 0.1198 - val_accuracy: 0.8976 - val_loss: 0.3617\n",
      "Epoch 20/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9616 - loss: 0.0939 - val_accuracy: 0.8933 - val_loss: 0.3986\n",
      "\u001b[1m 1/28\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 1s/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-14 14:22:28.710855: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_18', 416 bytes spill stores, 416 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 98ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-14 14:22:29,976] Trial 0 finished with value: -0.8725824800910125 and parameters: {'n_layers': 3, 'n_units_l0': 464, 'n_units_l1': 421, 'n_units_l2': 430, 'dropout_rate': 0.35910050292057816, 'learning_rate': 0.00036925626800611767}. Best is trial 0 with value: -0.8725824800910125.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7666 - loss: 0.6460 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-14 14:22:35.542450: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_26', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 45ms/step - accuracy: 0.7671 - loss: 0.6447 - val_accuracy: 0.8762 - val_loss: 0.2820\n",
      "Epoch 2/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8690 - loss: 0.3239 - val_accuracy: 0.8791 - val_loss: 0.3163\n",
      "Epoch 3/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8716 - loss: 0.2776 - val_accuracy: 0.8876 - val_loss: 0.2473\n",
      "Epoch 4/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9009 - loss: 0.2503 - val_accuracy: 0.8905 - val_loss: 0.2803\n",
      "Epoch 5/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8968 - loss: 0.2292 - val_accuracy: 0.8962 - val_loss: 0.2526\n",
      "Epoch 6/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9211 - loss: 0.1982 - val_accuracy: 0.8962 - val_loss: 0.2712\n",
      "Epoch 7/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9070 - loss: 0.2083 - val_accuracy: 0.8990 - val_loss: 0.2606\n",
      "Epoch 8/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9162 - loss: 0.1909 - val_accuracy: 0.8848 - val_loss: 0.2681\n",
      "Epoch 9/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9331 - loss: 0.1582 - val_accuracy: 0.8905 - val_loss: 0.3267\n",
      "Epoch 10/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9254 - loss: 0.1672 - val_accuracy: 0.8905 - val_loss: 0.3122\n",
      "Epoch 11/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9358 - loss: 0.1426 - val_accuracy: 0.9018 - val_loss: 0.3130\n",
      "Epoch 12/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9382 - loss: 0.1384 - val_accuracy: 0.8905 - val_loss: 0.3524\n",
      "Epoch 13/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9310 - loss: 0.1548 - val_accuracy: 0.8834 - val_loss: 0.3356\n",
      "Epoch 14/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9355 - loss: 0.1592 - val_accuracy: 0.9004 - val_loss: 0.3117\n",
      "Epoch 15/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9400 - loss: 0.1390 - val_accuracy: 0.8876 - val_loss: 0.3168\n",
      "Epoch 16/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9512 - loss: 0.1174 - val_accuracy: 0.8962 - val_loss: 0.3430\n",
      "Epoch 17/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9450 - loss: 0.1174 - val_accuracy: 0.8905 - val_loss: 0.3540\n",
      "Epoch 18/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9505 - loss: 0.1282 - val_accuracy: 0.8819 - val_loss: 0.3301\n",
      "Epoch 19/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9531 - loss: 0.1248 - val_accuracy: 0.8990 - val_loss: 0.3391\n",
      "Epoch 20/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9419 - loss: 0.1196 - val_accuracy: 0.8933 - val_loss: 0.4023\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-14 14:22:40,516] Trial 1 finished with value: -0.8748577929465301 and parameters: {'n_layers': 2, 'n_units_l0': 221, 'n_units_l1': 157, 'dropout_rate': 0.26929612980640505, 'learning_rate': 0.003023162615947716}. Best is trial 1 with value: -0.8748577929465301.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 48ms/step - accuracy: 0.7684 - loss: 0.7665 - val_accuracy: 0.8407 - val_loss: 0.3715\n",
      "Epoch 2/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8369 - loss: 0.4593 - val_accuracy: 0.8905 - val_loss: 0.2787\n",
      "Epoch 3/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8925 - loss: 0.3157 - val_accuracy: 0.8990 - val_loss: 0.2447\n",
      "Epoch 4/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8678 - loss: 0.2989 - val_accuracy: 0.8876 - val_loss: 0.2494\n",
      "Epoch 5/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8929 - loss: 0.2622 - val_accuracy: 0.8834 - val_loss: 0.2831\n",
      "Epoch 6/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8988 - loss: 0.2536 - val_accuracy: 0.8962 - val_loss: 0.2577\n",
      "Epoch 7/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9038 - loss: 0.2347 - val_accuracy: 0.9004 - val_loss: 0.2411\n",
      "Epoch 8/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8952 - loss: 0.2386 - val_accuracy: 0.8962 - val_loss: 0.2508\n",
      "Epoch 9/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9019 - loss: 0.2466 - val_accuracy: 0.9004 - val_loss: 0.2605\n",
      "Epoch 10/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9055 - loss: 0.2291 - val_accuracy: 0.8791 - val_loss: 0.3397\n",
      "Epoch 11/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8908 - loss: 0.2940 - val_accuracy: 0.8890 - val_loss: 0.3007\n",
      "Epoch 12/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9105 - loss: 0.2083 - val_accuracy: 0.8748 - val_loss: 0.3239\n",
      "Epoch 13/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8969 - loss: 0.2781 - val_accuracy: 0.8506 - val_loss: 0.4065\n",
      "Epoch 14/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8907 - loss: 0.3183 - val_accuracy: 0.8848 - val_loss: 0.2653\n",
      "Epoch 15/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9243 - loss: 0.1943 - val_accuracy: 0.9018 - val_loss: 0.2879\n",
      "Epoch 16/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9295 - loss: 0.1665 - val_accuracy: 0.8862 - val_loss: 0.3071\n",
      "Epoch 17/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9183 - loss: 0.1902 - val_accuracy: 0.8962 - val_loss: 0.2503\n",
      "Epoch 18/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9211 - loss: 0.1762 - val_accuracy: 0.8976 - val_loss: 0.3232\n",
      "Epoch 19/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9244 - loss: 0.1865 - val_accuracy: 0.8819 - val_loss: 0.3196\n",
      "Epoch 20/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9232 - loss: 0.1846 - val_accuracy: 0.8947 - val_loss: 0.3075\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-14 14:22:51,795] Trial 2 finished with value: -0.8794084186575654 and parameters: {'n_layers': 2, 'n_units_l0': 291, 'n_units_l1': 386, 'dropout_rate': 0.43607258917966674, 'learning_rate': 0.0038200628719317737}. Best is trial 2 with value: -0.8794084186575654.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.8219 - loss: 0.4034 - val_accuracy: 0.8876 - val_loss: 0.2841\n",
      "Epoch 2/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8993 - loss: 0.2713 - val_accuracy: 0.8933 - val_loss: 0.2793\n",
      "Epoch 3/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9146 - loss: 0.2165 - val_accuracy: 0.8933 - val_loss: 0.2811\n",
      "Epoch 4/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9158 - loss: 0.2006 - val_accuracy: 0.8890 - val_loss: 0.2984\n",
      "Epoch 5/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9060 - loss: 0.2073 - val_accuracy: 0.9047 - val_loss: 0.2563\n",
      "Epoch 6/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9260 - loss: 0.1706 - val_accuracy: 0.8947 - val_loss: 0.2837\n",
      "Epoch 7/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9425 - loss: 0.1528 - val_accuracy: 0.8819 - val_loss: 0.3130\n",
      "Epoch 8/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9400 - loss: 0.1696 - val_accuracy: 0.8962 - val_loss: 0.2882\n",
      "Epoch 9/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9411 - loss: 0.1515 - val_accuracy: 0.8962 - val_loss: 0.2825\n",
      "Epoch 10/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9536 - loss: 0.1217 - val_accuracy: 0.8976 - val_loss: 0.3006\n",
      "Epoch 11/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9629 - loss: 0.1097 - val_accuracy: 0.8876 - val_loss: 0.2985\n",
      "Epoch 12/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9624 - loss: 0.0958 - val_accuracy: 0.8933 - val_loss: 0.2903\n",
      "Epoch 13/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9688 - loss: 0.0894 - val_accuracy: 0.8848 - val_loss: 0.3272\n",
      "Epoch 14/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9651 - loss: 0.0992 - val_accuracy: 0.9033 - val_loss: 0.3471\n",
      "Epoch 15/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9612 - loss: 0.1102 - val_accuracy: 0.8933 - val_loss: 0.3159\n",
      "Epoch 16/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9599 - loss: 0.0964 - val_accuracy: 0.8976 - val_loss: 0.3418\n",
      "Epoch 17/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9696 - loss: 0.0875 - val_accuracy: 0.8933 - val_loss: 0.3396\n",
      "Epoch 18/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9772 - loss: 0.0735 - val_accuracy: 0.8990 - val_loss: 0.3577\n",
      "Epoch 19/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9784 - loss: 0.0658 - val_accuracy: 0.8848 - val_loss: 0.4211\n",
      "Epoch 20/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9710 - loss: 0.0851 - val_accuracy: 0.8947 - val_loss: 0.3569\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-14 14:22:57,567] Trial 3 finished with value: -0.8691695108077361 and parameters: {'n_layers': 1, 'n_units_l0': 332, 'dropout_rate': 0.2072418291539615, 'learning_rate': 0.0007057160803438832}. Best is trial 2 with value: -0.8794084186575654.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 0.7688 - loss: 0.9398 - val_accuracy: 0.8634 - val_loss: 0.3430\n",
      "Epoch 2/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8529 - loss: 0.3784 - val_accuracy: 0.8535 - val_loss: 0.2938\n",
      "Epoch 3/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8629 - loss: 0.3448 - val_accuracy: 0.8720 - val_loss: 0.2749\n",
      "Epoch 4/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8649 - loss: 0.3373 - val_accuracy: 0.8933 - val_loss: 0.2861\n",
      "Epoch 5/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8693 - loss: 0.3174 - val_accuracy: 0.8606 - val_loss: 0.3152\n",
      "Epoch 6/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8640 - loss: 0.3113 - val_accuracy: 0.8691 - val_loss: 0.3521\n",
      "Epoch 7/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8682 - loss: 0.3174 - val_accuracy: 0.8805 - val_loss: 0.2975\n",
      "Epoch 8/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8690 - loss: 0.3363 - val_accuracy: 0.8649 - val_loss: 0.2873\n",
      "Epoch 9/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8739 - loss: 0.3283 - val_accuracy: 0.8819 - val_loss: 0.3595\n",
      "Epoch 10/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8816 - loss: 0.2916 - val_accuracy: 0.8620 - val_loss: 0.2924\n",
      "Epoch 11/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8819 - loss: 0.2945 - val_accuracy: 0.8677 - val_loss: 0.3107\n",
      "Epoch 12/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8514 - loss: 0.3540 - val_accuracy: 0.8691 - val_loss: 0.3283\n",
      "Epoch 13/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8675 - loss: 0.3236 - val_accuracy: 0.8762 - val_loss: 0.3007\n",
      "Epoch 14/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8848 - loss: 0.3122 - val_accuracy: 0.8791 - val_loss: 0.3388\n",
      "Epoch 15/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8930 - loss: 0.2737 - val_accuracy: 0.8905 - val_loss: 0.3222\n",
      "Epoch 16/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8736 - loss: 0.2967 - val_accuracy: 0.8620 - val_loss: 0.3229\n",
      "Epoch 17/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8855 - loss: 0.2954 - val_accuracy: 0.8677 - val_loss: 0.3832\n",
      "Epoch 18/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8855 - loss: 0.2896 - val_accuracy: 0.8777 - val_loss: 0.4549\n",
      "Epoch 19/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8718 - loss: 0.3578 - val_accuracy: 0.8734 - val_loss: 0.3213\n",
      "Epoch 20/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8749 - loss: 0.2836 - val_accuracy: 0.8592 - val_loss: 0.3351\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-14 14:23:07,177] Trial 4 finished with value: -0.8361774744027304 and parameters: {'n_layers': 2, 'n_units_l0': 106, 'n_units_l1': 183, 'dropout_rate': 0.4187088874001139, 'learning_rate': 0.00801713255722954}. Best is trial 2 with value: -0.8794084186575654.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_layers': 2, 'n_units_l0': 291, 'n_units_l1': 386, 'dropout_rate': 0.43607258917966674, 'learning_rate': 0.0038200628719317737}\n",
      "Epoch 1/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 25ms/step - accuracy: 0.7953 - loss: 0.8618 - val_accuracy: 0.8634 - val_loss: 0.3368\n",
      "Epoch 2/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8488 - loss: 0.3867 - val_accuracy: 0.8606 - val_loss: 0.2868\n",
      "Epoch 3/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8689 - loss: 0.3397 - val_accuracy: 0.8691 - val_loss: 0.2800\n",
      "Epoch 4/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8844 - loss: 0.2639 - val_accuracy: 0.8706 - val_loss: 0.2786\n",
      "Epoch 5/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8826 - loss: 0.2808 - val_accuracy: 0.8791 - val_loss: 0.2478\n",
      "Epoch 6/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8865 - loss: 0.2505 - val_accuracy: 0.8862 - val_loss: 0.2742\n",
      "Epoch 7/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8904 - loss: 0.2681 - val_accuracy: 0.8933 - val_loss: 0.2550\n",
      "Epoch 8/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9093 - loss: 0.2218 - val_accuracy: 0.8777 - val_loss: 0.2801\n",
      "Epoch 9/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9065 - loss: 0.2408 - val_accuracy: 0.8734 - val_loss: 0.2940\n",
      "Epoch 10/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9022 - loss: 0.2522 - val_accuracy: 0.9061 - val_loss: 0.2703\n",
      "Epoch 11/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9178 - loss: 0.1991 - val_accuracy: 0.9004 - val_loss: 0.2413\n",
      "Epoch 12/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9097 - loss: 0.2260 - val_accuracy: 0.8819 - val_loss: 0.3095\n",
      "Epoch 13/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8982 - loss: 0.2428 - val_accuracy: 0.8762 - val_loss: 0.2747\n",
      "Epoch 14/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9011 - loss: 0.2334 - val_accuracy: 0.8762 - val_loss: 0.2895\n",
      "Epoch 15/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9128 - loss: 0.2047 - val_accuracy: 0.8976 - val_loss: 0.2678\n",
      "Epoch 16/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9202 - loss: 0.1766 - val_accuracy: 0.8862 - val_loss: 0.3415\n",
      "Epoch 17/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9107 - loss: 0.2023 - val_accuracy: 0.9033 - val_loss: 0.2639\n",
      "Epoch 18/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9225 - loss: 0.1813 - val_accuracy: 0.8962 - val_loss: 0.3039\n",
      "Epoch 19/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9321 - loss: 0.1797 - val_accuracy: 0.8890 - val_loss: 0.3285\n",
      "Epoch 20/20\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9094 - loss: 0.1960 - val_accuracy: 0.8720 - val_loss: 0.3234\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.8501 - loss: 0.3969\n",
      "Test Accuracy: 0.8453\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8453\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['models/5_scaler.joblib']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Input\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_shape, layers, dropout_rate, learning_rate, device):\n",
    "        self.input_shape = input_shape\n",
    "        self.layers = layers\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.device = device\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Input(shape=(self.input_shape,)))\n",
    "        for layer_size in self.layers:\n",
    "            model.add(Dense(layer_size, activation='relu'))\n",
    "            model.add(Dropout(self.dropout_rate))\n",
    "        model.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification\n",
    "        return model\n",
    "\n",
    "    def compile_model(self):\n",
    "        optimizer = Adam(learning_rate=self.learning_rate)\n",
    "        self.model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])  # Use binary crossentropy for binary classification\n",
    "\n",
    "    def train_model(self, X_train, y_train, epochs=20, batch_size=32, validation_split=0.2):\n",
    "        with tf.device(self.device):\n",
    "            history = self.model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=validation_split)\n",
    "        return history\n",
    "\n",
    "    def evaluate_model(self, X_test, y_test):\n",
    "        with tf.device(self.device):\n",
    "            loss, accuracy = self.model.evaluate(X_test, y_test)\n",
    "        return loss, accuracy\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        with tf.device(self.device):\n",
    "            predictions = self.model.predict(X_test)\n",
    "        return (predictions > 0.5).astype(int).flatten()  # Convert probabilities to binary predictions\n",
    "\n",
    "    def calculate_accuracy(self, y_test, predictions):\n",
    "        return accuracy_score(y_test, predictions)\n",
    "    \n",
    "    def save_model(self, filename):\n",
    "        self.model.save(filename)\n",
    "\n",
    "    @classmethod\n",
    "    def load_model(cls, filename, input_shape, device):\n",
    "        loaded_model = tf.keras.models.load_model(filename)\n",
    "        nn = cls(input_shape, [], 0, 0, device)  # Dummy values for layers, dropout_rate, and learning_rate\n",
    "        nn.model = loaded_model\n",
    "        return nn\n",
    "\n",
    "def objective(trial):\n",
    "    layers = []\n",
    "    for i in range(trial.suggest_int('n_layers', 1, 3)):\n",
    "        layers.append(trial.suggest_int(f'n_units_l{i}', 64, 512))\n",
    "    \n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.2, 0.5)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2)\n",
    "    \n",
    "    nn = NeuralNetwork(input_shape=X_train.shape[1], device=device, layers=layers, dropout_rate=dropout_rate, learning_rate=learning_rate)\n",
    "    nn.compile_model()\n",
    "    \n",
    "    nn.train_model(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2)\n",
    "    \n",
    "    predictions = nn.predict(X_test)\n",
    "    accuracy = nn.calculate_accuracy(y_test, predictions)\n",
    "    \n",
    "    return -accuracy  # We want to maximize accuracy, so minimize negative accuracy\n",
    "\n",
    "# Assuming combined_gdf is already loaded\n",
    "combined_gdf_filtered = gdf[['geometry','embeddings_delta','flood']].dropna()\n",
    "\n",
    "# Flatten the nested arrays\n",
    "def flatten_embeddings(embedding):\n",
    "    if isinstance(embedding, np.ndarray) and embedding.ndim > 0:\n",
    "        return np.concatenate(embedding) if embedding.ndim > 1 else embedding.flatten()\n",
    "    else:\n",
    "        return np.array([])  # Handle cases where embedding might be zero-dimensional or empty\n",
    "\n",
    "X = np.array([flatten_embeddings(x) for x in combined_gdf_filtered['embeddings_delta'] if flatten_embeddings(x).size > 0])\n",
    "y = combined_gdf_filtered['flood']\n",
    "\n",
    "# Ensure that X and y have matching lengths after filtering\n",
    "X = np.array([x for i, x in enumerate(X) if flatten_embeddings(combined_gdf_filtered['embeddings_delta'].iloc[i]).size > 0])\n",
    "y = y.iloc[:X.shape[0]]  # Adjust y to have the same number of samples as X\n",
    "\n",
    "# Convert y to binary labels if it's not already\n",
    "y = (y > 0).astype(int)  # Assuming 'flood' is a numeric value, convert to binary (e.g., flood/no flood)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "device = '/GPU:0' if tf.config.list_physical_devices('GPU') else '/CPU:0'\n",
    "\n",
    "# Optimize the hyperparameters\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=5)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(study.best_params)\n",
    "\n",
    "# Example usage with the best hyperparameters\n",
    "best_params = study.best_params\n",
    "layers = [best_params[f'n_units_l{i}'] for i in range(best_params['n_layers'])]\n",
    "dropout_rate = best_params['dropout_rate']\n",
    "learning_rate = best_params['learning_rate']\n",
    "\n",
    "nn = NeuralNetwork(input_shape=X_train.shape[1], device=device, layers=layers, dropout_rate=dropout_rate, learning_rate=learning_rate)\n",
    "nn.compile_model()\n",
    "history = nn.train_model(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2)\n",
    "loss, accuracy = nn.evaluate_model(X_test, y_test)\n",
    "print(f'Test Accuracy: {accuracy:.4f}')\n",
    "predictions = nn.predict(X_test)\n",
    "accuracy = nn.calculate_accuracy(y_test, predictions)\n",
    "print(f'Test Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Save the model\n",
    "nn.save_model('models/task_5_model.h5')\n",
    "\n",
    "# Save scaler\n",
    "joblib.dump(scaler, 'models/5_scaler.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>POLYGON ((-104.41591 32.61522, -104.41591 32.1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            geometry\n",
       "0  POLYGON ((-104.41591 32.61522, -104.41591 32.1..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "# Load the GeoJSON file\n",
    "geojson_path = 'test_data/challenge_5_bb.geojson'\n",
    "gdf = gpd.read_file(geojson_path)\n",
    "gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>POLYGON ((554799.704 3608782.260, 555059.104 3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            geometry\n",
       "0  POLYGON ((554799.704 3608782.260, 555059.104 3..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyproj\n",
    "\n",
    "def get_utm_zone(longitude):\n",
    "    return int((longitude + 180) / 6) + 1\n",
    "\n",
    "# Get the bounds of the geometry\n",
    "minx, miny, maxx, maxy = gdf.geometry.bounds.iloc[0]\n",
    "\n",
    "# Calculate UTM zone\n",
    "utm_zone = get_utm_zone(minx)\n",
    "\n",
    "# Check for a suitable projection using pyproj\n",
    "proj = pyproj.Proj(proj='utm', zone=utm_zone, ellps='WGS84')\n",
    "\n",
    "# Get the corresponding EPSG code for the UTM zone using pyproj\n",
    "utm_crs = pyproj.CRS(f\"+proj=utm +zone={utm_zone} +datum=WGS84\")\n",
    "epsg_code = utm_crs.to_epsg()\n",
    "\n",
    "# Reproject the GeoDataFrame to the chosen EPSG code\n",
    "gdf = gdf.to_crs(epsg=epsg_code)\n",
    "gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>POINT (554799.704 3561297.142)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>POINT (557359.704 3561297.142)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>POINT (559919.704 3561297.142)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>POINT (562479.704 3561297.142)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>POINT (565039.704 3561297.142)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>POINT (595759.704 3607377.142)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>POINT (598319.704 3607377.142)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>POINT (600879.704 3607377.142)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>POINT (603439.704 3607377.142)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>POINT (605999.704 3607377.142)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>399 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           geometry\n",
       "0    POINT (554799.704 3561297.142)\n",
       "1    POINT (557359.704 3561297.142)\n",
       "2    POINT (559919.704 3561297.142)\n",
       "3    POINT (562479.704 3561297.142)\n",
       "4    POINT (565039.704 3561297.142)\n",
       "..                              ...\n",
       "394  POINT (595759.704 3607377.142)\n",
       "395  POINT (598319.704 3607377.142)\n",
       "396  POINT (600879.704 3607377.142)\n",
       "397  POINT (603439.704 3607377.142)\n",
       "398  POINT (605999.704 3607377.142)\n",
       "\n",
       "[399 rows x 1 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create a grid of points 5120m apart\n",
    "x = np.arange(gdf.total_bounds[0], gdf.total_bounds[2], 2560)\n",
    "y = np.arange(gdf.total_bounds[1], gdf.total_bounds[3], 2560)\n",
    "xx, yy = np.meshgrid(x, y)\n",
    "points = np.vstack([xx.ravel(), yy.ravel()]).T\n",
    "\n",
    "grid = gpd.GeoDataFrame(geometry=gpd.points_from_xy(points[:, 0], points[:, 1], crs=gdf.crs))\n",
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 399/399 [11:14<00:00,  1.69s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>embeddings_2017</th>\n",
       "      <th>embeddings_2018</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-104.418657</td>\n",
       "      <td>32.186869</td>\n",
       "      <td>[[0.024514427, -0.019557817, -0.015843777, 0.1...</td>\n",
       "      <td>[[0.0042556943, -0.029356357, -0.045840662, 0....</td>\n",
       "      <td>POINT (-104.41866 32.18687)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-104.391500</td>\n",
       "      <td>32.186742</td>\n",
       "      <td>[[0.022209732, -0.037956085, 0.018563386, 0.18...</td>\n",
       "      <td>[[0.007970055, -0.03960701, -0.0102273, 0.1606...</td>\n",
       "      <td>POINT (-104.39150 32.18674)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-104.364344</td>\n",
       "      <td>32.186608</td>\n",
       "      <td>[[0.06547231, -0.0403255, 0.03680599, 0.102555...</td>\n",
       "      <td>[[0.05164792, -0.038914327, 0.033683784, 0.115...</td>\n",
       "      <td>POINT (-104.36434 32.18661)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-104.337188</td>\n",
       "      <td>32.186469</td>\n",
       "      <td>[[-0.042644504, 0.0076282974, 0.07318993, 0.11...</td>\n",
       "      <td>[[-0.040943988, 0.023629641, 0.06421219, 0.124...</td>\n",
       "      <td>POINT (-104.33719 32.18647)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-104.310032</td>\n",
       "      <td>32.186323</td>\n",
       "      <td>[[-0.014203708, 0.038455915, 0.07248832, 0.094...</td>\n",
       "      <td>[[-0.003047574, 0.045992475, 0.06563147, 0.110...</td>\n",
       "      <td>POINT (-104.31003 32.18632)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          lon        lat                                    embeddings_2017  \\\n",
       "0 -104.418657  32.186869  [[0.024514427, -0.019557817, -0.015843777, 0.1...   \n",
       "1 -104.391500  32.186742  [[0.022209732, -0.037956085, 0.018563386, 0.18...   \n",
       "2 -104.364344  32.186608  [[0.06547231, -0.0403255, 0.03680599, 0.102555...   \n",
       "3 -104.337188  32.186469  [[-0.042644504, 0.0076282974, 0.07318993, 0.11...   \n",
       "4 -104.310032  32.186323  [[-0.014203708, 0.038455915, 0.07248832, 0.094...   \n",
       "\n",
       "                                     embeddings_2018  \\\n",
       "0  [[0.0042556943, -0.029356357, -0.045840662, 0....   \n",
       "1  [[0.007970055, -0.03960701, -0.0102273, 0.1606...   \n",
       "2  [[0.05164792, -0.038914327, 0.033683784, 0.115...   \n",
       "3  [[-0.040943988, 0.023629641, 0.06421219, 0.124...   \n",
       "4  [[-0.003047574, 0.045992475, 0.06563147, 0.110...   \n",
       "\n",
       "                      geometry  \n",
       "0  POINT (-104.41866 32.18687)  \n",
       "1  POINT (-104.39150 32.18674)  \n",
       "2  POINT (-104.36434 32.18661)  \n",
       "3  POINT (-104.33719 32.18647)  \n",
       "4  POINT (-104.31003 32.18632)  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import pystac_client\n",
    "import stackstac\n",
    "import torch\n",
    "from torchvision import transforms as v2\n",
    "from box import Box\n",
    "import yaml\n",
    "import math\n",
    "from rasterio.enums import Resampling\n",
    "from tqdm import tqdm\n",
    "import rasterio\n",
    "import warnings\n",
    "import os\n",
    "import numpy as np\n",
    "import rioxarray  # Make sure to import rioxarray to extend xarray\n",
    "\n",
    "from src.model import ClayMAEModule\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "STAC_API = \"https://earth-search.aws.element84.com/v1\"\n",
    "COLLECTION = \"sentinel-2-l2a\"\n",
    "\n",
    "# Load the model and metadata\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ckpt = \"https://clay-model-ckpt.s3.amazonaws.com/v0.5.7/mae_v0.5.7_epoch-13_val-loss-0.3098.ckpt\"\n",
    "torch.set_default_device(device)\n",
    "\n",
    "torch.cuda.empty_cache()  # Clear GPU cache\n",
    "\n",
    "# Assuming grid is a GeoDataFrame with the points\n",
    "points = grid.to_crs(\"EPSG:4326\").geometry.apply(lambda x: (x.x, x.y)).tolist()\n",
    "\n",
    "model = ClayMAEModule.load_from_checkpoint(\n",
    "    ckpt, metadata_path=\"configs/metadata.yaml\", shuffle=False, mask_ratio=0\n",
    ")\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "\n",
    "metadata = Box(yaml.safe_load(open(\"configs/metadata.yaml\")))\n",
    "\n",
    "# Function to normalize timestamp\n",
    "def normalize_timestamp(date):\n",
    "    week = date.isocalendar().week * 2 * np.pi / 52\n",
    "    hour = date.hour * 2 * np.pi / 24\n",
    "    return (math.sin(week), math.cos(week)), (math.sin(hour), math.cos(hour))\n",
    "\n",
    "# Function to normalize lat/lon\n",
    "def normalize_latlon(lat, lon):\n",
    "    lat = lat * np.pi / 180\n",
    "    lon = lon * np.pi / 180\n",
    "    return (math.sin(lat), math.cos(lat)), (math.sin(lon), math.cos(lon))\n",
    "\n",
    "def to_device(data, device):\n",
    "    if isinstance(data, torch.Tensor):\n",
    "        return data.to(device)\n",
    "    elif isinstance(data, dict):\n",
    "        return {k: to_device(v, device) for k, v in data.items()}\n",
    "    elif isinstance(data, list):\n",
    "        return [to_device(v, device) for v in data]\n",
    "    return data\n",
    "\n",
    "def process_point(lon, lat, model, metadata, year, device, j):\n",
    "    model.to(device)  # Ensure the model is on the correct device\n",
    "    catalog = pystac_client.Client.open(STAC_API)\n",
    "    search = catalog.search(\n",
    "        collections=[COLLECTION],\n",
    "        datetime=f\"{year}-06-04/{year}-07-01\",\n",
    "        bbox=(lon - 1e-5, lat - 1e-5, lon + 1e-5, lat + 1e-5),\n",
    "        max_items=10,\n",
    "        query={\"eo:cloud_cover\": {\"lt\": 80}},\n",
    "    )\n",
    "\n",
    "    all_items = search.get_all_items()\n",
    "    items = list(all_items)\n",
    "    if not items:\n",
    "        return None\n",
    "    \n",
    "    items = sorted(items, key=lambda x: x.properties.get('eo:cloud_cover', float('inf')))\n",
    "    lowest_cloud_item = items[0]\n",
    "\n",
    "    epsg = lowest_cloud_item.properties[\"proj:epsg\"]\n",
    "\n",
    "    poidf = gpd.GeoDataFrame(\n",
    "        pd.DataFrame(),\n",
    "        crs=\"EPSG:4326\",\n",
    "        geometry=[Point(lon, lat)],\n",
    "    ).to_crs(epsg)\n",
    "\n",
    "    coords = poidf.iloc[0].geometry.coords[0]\n",
    "\n",
    "    size = 256\n",
    "    gsd = 10\n",
    "    bounds = (\n",
    "        coords[0] - (size * gsd) // 2,\n",
    "        coords[1] - (size * gsd) // 2,\n",
    "        coords[0] + (size * gsd) // 2,\n",
    "        coords[1] + (size * gsd) // 2,\n",
    "    )\n",
    "\n",
    "    stack = stackstac.stack(\n",
    "        lowest_cloud_item,\n",
    "        bounds=bounds,\n",
    "        snap_bounds=False,\n",
    "        epsg=epsg,\n",
    "        resolution=gsd,\n",
    "        dtype=\"float32\",\n",
    "        rescale=False,\n",
    "        fill_value=0,\n",
    "        assets=[\"blue\", \"green\", \"red\", \"nir\"],\n",
    "        resampling=Resampling.nearest,\n",
    "    )\n",
    "\n",
    "    stack = stack.compute()\n",
    "\n",
    "    items = []\n",
    "    dates = []\n",
    "    for item in all_items:\n",
    "        if item.datetime.date() not in dates:\n",
    "            items.append(item)\n",
    "            dates.append(item.datetime.date())\n",
    "\n",
    "    platform = \"sentinel-2-l2a\"\n",
    "    mean = []\n",
    "    std = []\n",
    "    waves = []\n",
    "    for band in stack.band:\n",
    "        mean.append(metadata[platform].bands.mean[str(band.values)])\n",
    "        std.append(metadata[platform].bands.std[str(band.values)])\n",
    "        waves.append(metadata[platform].bands.wavelength[str(band.values)])\n",
    "\n",
    "    transform = v2.Compose([v2.Normalize(mean=mean, std=std)])\n",
    "\n",
    "    datetimes = stack.time.values.astype(\"datetime64[s]\").tolist()\n",
    "    times = [normalize_timestamp(dat) for dat in datetimes]\n",
    "    week_norm = [dat[0] for dat in times]\n",
    "    hour_norm = [dat[1] for dat in times]\n",
    "\n",
    "    latlons = [normalize_latlon(lat, lon)] * len(times)\n",
    "    lat_norm = [dat[0] for dat in latlons]\n",
    "    lon_norm = [dat[1] for dat in latlons]\n",
    "\n",
    "    pixels = torch.from_numpy(stack.data.astype(np.float32)).to(device)\n",
    "    pixels = transform(pixels)\n",
    "\n",
    "    batch_size = 16\n",
    "    num_batches = math.ceil(len(stack) / batch_size)\n",
    "    \n",
    "    embeddings_list = []\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, len(stack))\n",
    "        \n",
    "        batch_pixels = pixels[start_idx:end_idx].to(device)\n",
    "        batch_time = torch.tensor(np.hstack((week_norm, hour_norm))[start_idx:end_idx], dtype=torch.float32).to(device)\n",
    "        batch_latlon = torch.tensor(np.hstack((lat_norm, lon_norm))[start_idx:end_idx], dtype=torch.float32).to(device)\n",
    "        \n",
    "        batch_datacube = {\n",
    "            \"platform\": platform,\n",
    "            \"time\": batch_time,\n",
    "            \"latlon\": batch_latlon,\n",
    "            \"pixels\": batch_pixels,\n",
    "            \"gsd\": torch.tensor(stack.gsd.values).to(device),\n",
    "            \"waves\": torch.tensor(waves).to(device),\n",
    "        }\n",
    "\n",
    "        batch_datacube = to_device(batch_datacube, device)\n",
    "\n",
    "        try:\n",
    "            model = model.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                unmsk_patch, _, _, _ = model.model.encoder(batch_datacube)\n",
    "            batch_embeddings = unmsk_patch[:, 0, :].cpu().numpy()\n",
    "            embeddings_list.append(batch_embeddings)\n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e):\n",
    "                print(f\"GPU OOM for point ({lon}, {lat}), batch {i+1}/{num_batches}. Trying CPU...\")\n",
    "                device = torch.device(\"cpu\")\n",
    "                batch_datacube = to_device(batch_datacube, device)\n",
    "                model = model.to(device)\n",
    "                with torch.no_grad():\n",
    "                    unmsk_patch, _, _, _ = model.model.encoder(batch_datacube)\n",
    "                batch_embeddings = unmsk_patch[:, 0, :].numpy()\n",
    "                embeddings_list.append(batch_embeddings)\n",
    "                device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "    embeddings = np.concatenate(embeddings_list, axis=0)\n",
    "    return embeddings\n",
    "\n",
    "# Initialize an empty dictionary to store results for both years\n",
    "results_dict = {\"lon\": [], \"lat\": [], \"embeddings_2017\": [], \"embeddings_2018\": []}\n",
    "\n",
    "# Specify the years for the datetime range in the search\n",
    "years = [2017, 2018]\n",
    "\n",
    "# Iterate through the points and process each one for both years\n",
    "for i, point in enumerate(tqdm(points)):\n",
    "    lon, lat = point\n",
    "    results_dict[\"lon\"].append(lon)\n",
    "    results_dict[\"lat\"].append(lat)\n",
    "    \n",
    "    for year in years:\n",
    "        embeddings = process_point(lon, lat, model, metadata, year, device, i)\n",
    "        if embeddings is not None:\n",
    "            results_dict[f\"embeddings_{year}\"].append(embeddings)\n",
    "        else:\n",
    "            results_dict[f\"embeddings_{year}\"].append(None)\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "df = pd.DataFrame(results_dict)\n",
    "\n",
    "# Convert to a GeoDataFrame\n",
    "gdf_results = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.lon, df.lat))\n",
    "\n",
    "# Output the resulting GeoDataFrame\n",
    "gdf_results.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_copy = gdf_results.copy()\n",
    "gdf_copy[\"embeddings_2017\"] = [embedding.flatten() if embedding is not None and embedding.size > 0 else None for embedding in gdf_results[\"embeddings_2017\"]]\n",
    "gdf_copy[\"embeddings_2018\"] = [embedding.flatten() if embedding is not None and embedding.size > 0 else None for embedding in gdf_results[\"embeddings_2018\"]]\n",
    "\n",
    "gdf_copy.to_parquet(\"test_data/challenge_5.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Raster 1: 100%|██████████| 399/399 [02:39<00:00,  2.51it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'flood'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/envs/claymodel-latest-v2/lib/python3.11/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'flood'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m gdf_copy[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mflood\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m raster_values\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Set values greater than 1 to NaN for raster_value_1\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m gdf_copy\u001b[38;5;241m.\u001b[39mloc[\u001b[43mgdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mflood\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mflood\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnan\n\u001b[1;32m     41\u001b[0m gdf_copy[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membeddings_delta\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m gdf_copy[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membeddings_2018\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m-\u001b[39m gdf_copy[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membeddings_2017\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     43\u001b[0m gdf_copy\u001b[38;5;241m.\u001b[39mto_parquet(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_data/challenge_5.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/claymodel-latest-v2/lib/python3.11/site-packages/geopandas/geodataframe.py:1459\u001b[0m, in \u001b[0;36mGeoDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1453\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m   1454\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1455\u001b[0m \u001b[38;5;124;03m    If the result is a column containing only 'geometry', return a\u001b[39;00m\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;124;03m    GeoSeries. If it's a DataFrame with any columns of GeometryDtype,\u001b[39;00m\n\u001b[1;32m   1457\u001b[0m \u001b[38;5;124;03m    return a GeoDataFrame.\u001b[39;00m\n\u001b[1;32m   1458\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1459\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1460\u001b[0m     \u001b[38;5;66;03m# Custom logic to avoid waiting for pandas GH51895\u001b[39;00m\n\u001b[1;32m   1461\u001b[0m     \u001b[38;5;66;03m# result is not geometry dtype for multi-indexes\u001b[39;00m\n\u001b[1;32m   1462\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1463\u001b[0m         pd\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mtypes\u001b[38;5;241m.\u001b[39mis_scalar(key)\n\u001b[1;32m   1464\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1467\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_geometry_type(result)\n\u001b[1;32m   1468\u001b[0m     ):\n",
      "File \u001b[0;32m/opt/conda/envs/claymodel-latest-v2/lib/python3.11/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/conda/envs/claymodel-latest-v2/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'flood'"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import rasterio\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import rioxarray\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Load your GeoDataFrame\n",
    "# gdf = gpd.read_file('your_geopandas_dataframe.gpkg')\n",
    "\n",
    "# Load the TIFF files as xarray Datasets\n",
    "raster_path = 'test_data/flood_WM_S1A_IW_GRDH_1SDV_20180604T125505_20180604T125530_022207_026702_642A.tif'\n",
    "\n",
    "# Convert the TIFFs to xarray datasets\n",
    "raster_xr = rioxarray.open_rasterio(raster_path)\n",
    "\n",
    "def get_raster_value_at_geometry_xr(geometry, raster_xr):\n",
    "    try:\n",
    "        clipped = raster_xr.rio.clip([geometry], raster_xr.rio.crs)\n",
    "        return float(clipped.mean().values)\n",
    "    except rioxarray.exceptions.NoDataInBounds:\n",
    "        return np.nan\n",
    "\n",
    "def process_row(geom, raster_xr):\n",
    "    return get_raster_value_at_geometry_xr(geom, raster_xr)\n",
    "\n",
    "# Parallel processing with joblib\n",
    "n_jobs = 12 # Use all available cores\n",
    "\n",
    "# Use tqdm with joblib for progress bar\n",
    "raster_values = Parallel(n_jobs=n_jobs)(\n",
    "    delayed(process_row)(geom, raster_xr) for geom in tqdm(gdf_copy['geometry'], desc=\"Processing Raster 1\"))\n",
    "\n",
    "# Assign the values back to the GeoDataFrame\n",
    "gdf_copy['flood'] = raster_values\n",
    "\n",
    "# Set values greater than 1 to NaN for raster_value_1\n",
    "gdf_copy.loc[gdf_copy['flood'] > 1, 'flood'] = np.nan\n",
    "\n",
    "gdf_copy['embeddings_delta'] = gdf_copy['embeddings_2018'] - gdf_copy['embeddings_2017']\n",
    "\n",
    "gdf_copy.to_parquet(\"test_data/challenge_5.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Input\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_shape, layers, dropout_rate, learning_rate, device):\n",
    "        self.input_shape = input_shape\n",
    "        self.layers = layers\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.device = device\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Input(shape=(self.input_shape,)))\n",
    "        for layer_size in self.layers:\n",
    "            model.add(Dense(layer_size, activation='relu'))\n",
    "            model.add(Dropout(self.dropout_rate))\n",
    "        model.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification\n",
    "        return model\n",
    "\n",
    "    def compile_model(self):\n",
    "        optimizer = Adam(learning_rate=self.learning_rate)\n",
    "        self.model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])  # Use binary crossentropy for binary classification\n",
    "\n",
    "    def train_model(self, X_train, y_train, epochs=20, batch_size=32, validation_split=0.2):\n",
    "        with tf.device(self.device):\n",
    "            history = self.model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=validation_split)\n",
    "        return history\n",
    "\n",
    "    def evaluate_model(self, X_test, y_test):\n",
    "        with tf.device(self.device):\n",
    "            loss, accuracy = self.model.evaluate(X_test, y_test)\n",
    "        return loss, accuracy\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        with tf.device(self.device):\n",
    "            predictions = self.model.predict(X_test)\n",
    "        return (predictions > 0.5).astype(int).flatten()  # Convert probabilities to binary predictions\n",
    "\n",
    "    def calculate_accuracy(self, y_test, predictions):\n",
    "        return accuracy_score(y_test, predictions)\n",
    "    \n",
    "    def save_model(self, filename):\n",
    "        self.model.save(filename)\n",
    "\n",
    "    @classmethod\n",
    "    def load_model(cls, filename, input_shape, device):\n",
    "        loaded_model = tf.keras.models.load_model(filename)\n",
    "        nn = cls(input_shape, [], 0, 0, device)  # Dummy values for layers, dropout_rate, and learning_rate\n",
    "        nn.model = loaded_model\n",
    "        return nn\n",
    "\n",
    "def objective(trial):\n",
    "    layers = []\n",
    "    for i in range(trial.suggest_int('n_layers', 1, 3)):\n",
    "        layers.append(trial.suggest_int(f'n_units_l{i}', 64, 512))\n",
    "    \n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.2, 0.5)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2)\n",
    "    \n",
    "    nn = NeuralNetwork(input_shape=X_train.shape[1], device=device, layers=layers, dropout_rate=dropout_rate, learning_rate=learning_rate)\n",
    "    nn.compile_model()\n",
    "    \n",
    "    nn.train_model(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2)\n",
    "    \n",
    "    predictions = nn.predict(X_test)\n",
    "    accuracy = nn.calculate_accuracy(y_test, predictions)\n",
    "    \n",
    "    return -accuracy  # We want to maximize accuracy, so minimize negative accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1723648156.989605   30624 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1723648156.991492   30624 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1723648156.992914   30624 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1723648157.000614   30624 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1723648157.002048   30624 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1723648157.003477   30624 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-14 15:09:17.004875: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 17909 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1723648157.364934   31258 service.cc:146] XLA service 0x7f1ee8004550 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1723648157.364979   31258 service.cc:154]   StreamExecutor device (0): NVIDIA A10G, Compute Capability 8.6\n",
      "2024-08-14 15:09:17.369866: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-08-14 15:09:17.385674: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 1/13\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 2s/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1723648159.281550   31258 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 124ms/step\n"
     ]
    }
   ],
   "source": [
    "# Detect if GPU is available\n",
    "device = '/GPU:0' if tf.config.list_physical_devices('GPU') else '/CPU:0'\n",
    "\n",
    "# Load the model\n",
    "loaded_nn = NeuralNetwork.load_model('models/task_5_model.h5', input_shape=768, device=device)\n",
    "\n",
    "# Prepare your new data (assuming it's in the same format as your training data)\n",
    "new_data = np.squeeze(gdf_copy['embeddings_delta'].tolist())\n",
    "new_data = pd.DataFrame(new_data)  # Ensure the new data is in DataFrame format\n",
    "\n",
    "# Make predictions\n",
    "new_predictions = loaded_nn.predict(new_data)\n",
    "\n",
    "gdf_copy['pred_flood'] = new_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.6290726817042607\n"
     ]
    }
   ],
   "source": [
    "print(\"Test set accuracy:\", np.mean(gdf_copy['pred_flood']==gdf_copy['flood']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "claymodel-latest-v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
