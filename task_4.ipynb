{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>POLYGON ((115.18666 37.93093, 115.18666 36.670...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            geometry\n",
       "0  POLYGON ((115.18666 37.93093, 115.18666 36.670..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import geopandas as gpd\n",
    "\n",
    "# # Load the GeoJSON file\n",
    "# geojson_path = 'train_data/challenge_4_bb.geojson'\n",
    "# gdf = gpd.read_file(geojson_path)\n",
    "# gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>POLYGON ((340637.814 4199701.949, 337955.761 4...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            geometry\n",
       "0  POLYGON ((340637.814 4199701.949, 337955.761 4..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import pyproj\n",
    "\n",
    "# def get_utm_zone(longitude):\n",
    "#     return int((longitude + 180) / 6) + 1\n",
    "\n",
    "# # Get the bounds of the geometry\n",
    "# minx, miny, maxx, maxy = gdf.geometry.bounds.iloc[0]\n",
    "\n",
    "# # Calculate UTM zone\n",
    "# utm_zone = get_utm_zone(minx)\n",
    "\n",
    "# # Check for a suitable projection using pyproj\n",
    "# proj = pyproj.Proj(proj='utm', zone=utm_zone, ellps='WGS84')\n",
    "\n",
    "# # Get the corresponding EPSG code for the UTM zone using pyproj\n",
    "# utm_crs = pyproj.CRS(f\"+proj=utm +zone={utm_zone} +datum=WGS84\")\n",
    "# epsg_code = utm_crs.to_epsg()\n",
    "\n",
    "# # Reproject the GeoDataFrame to the chosen EPSG code\n",
    "# gdf = gdf.to_crs(epsg=epsg_code)\n",
    "# gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>POINT (337955.761 4058385.040)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>POINT (340515.761 4058385.040)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>POINT (343075.761 4058385.040)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>POINT (345635.761 4058385.040)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>POINT (348195.761 4058385.040)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3859</th>\n",
       "      <td>POINT (501795.761 4199185.040)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3860</th>\n",
       "      <td>POINT (504355.761 4199185.040)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3861</th>\n",
       "      <td>POINT (506915.761 4199185.040)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3862</th>\n",
       "      <td>POINT (509475.761 4199185.040)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3863</th>\n",
       "      <td>POINT (512035.761 4199185.040)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3864 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            geometry\n",
       "0     POINT (337955.761 4058385.040)\n",
       "1     POINT (340515.761 4058385.040)\n",
       "2     POINT (343075.761 4058385.040)\n",
       "3     POINT (345635.761 4058385.040)\n",
       "4     POINT (348195.761 4058385.040)\n",
       "...                              ...\n",
       "3859  POINT (501795.761 4199185.040)\n",
       "3860  POINT (504355.761 4199185.040)\n",
       "3861  POINT (506915.761 4199185.040)\n",
       "3862  POINT (509475.761 4199185.040)\n",
       "3863  POINT (512035.761 4199185.040)\n",
       "\n",
       "[3864 rows x 1 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# # Create a grid of points 5120m apart\n",
    "# x = np.arange(gdf.total_bounds[0], gdf.total_bounds[2], 2560)\n",
    "# y = np.arange(gdf.total_bounds[1], gdf.total_bounds[3], 2560)\n",
    "# xx, yy = np.meshgrid(x, y)\n",
    "# points = np.vstack([xx.ravel(), yy.ravel()]).T\n",
    "\n",
    "# grid = gpd.GeoDataFrame(geometry=gpd.points_from_xy(points[:, 0], points[:, 1], crs=gdf.crs))\n",
    "# grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3864/3864 [53:44<00:00,  1.20it/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>embeddings</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>115.186981</td>\n",
       "      <td>36.657277</td>\n",
       "      <td>[[0.011127532, -0.12299517, -0.13298486, 0.186...</td>\n",
       "      <td>POINT (115.18698 36.65728)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>115.215611</td>\n",
       "      <td>36.657709</td>\n",
       "      <td>[[0.0035498408, -0.11508457, -0.13443036, 0.17...</td>\n",
       "      <td>POINT (115.21561 36.65771)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>115.244241</td>\n",
       "      <td>36.658135</td>\n",
       "      <td>[[0.005627362, -0.12565899, -0.14232168, 0.184...</td>\n",
       "      <td>POINT (115.24424 36.65814)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>115.272872</td>\n",
       "      <td>36.658554</td>\n",
       "      <td>[[0.019624209, -0.11788666, -0.14006627, 0.160...</td>\n",
       "      <td>POINT (115.27287 36.65855)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>115.301503</td>\n",
       "      <td>36.658966</td>\n",
       "      <td>[[0.030935973, -0.12229435, -0.13792522, 0.161...</td>\n",
       "      <td>POINT (115.30150 36.65897)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          lon        lat                                         embeddings  \\\n",
       "0  115.186981  36.657277  [[0.011127532, -0.12299517, -0.13298486, 0.186...   \n",
       "1  115.215611  36.657709  [[0.0035498408, -0.11508457, -0.13443036, 0.17...   \n",
       "2  115.244241  36.658135  [[0.005627362, -0.12565899, -0.14232168, 0.184...   \n",
       "3  115.272872  36.658554  [[0.019624209, -0.11788666, -0.14006627, 0.160...   \n",
       "4  115.301503  36.658966  [[0.030935973, -0.12229435, -0.13792522, 0.161...   \n",
       "\n",
       "                     geometry  \n",
       "0  POINT (115.18698 36.65728)  \n",
       "1  POINT (115.21561 36.65771)  \n",
       "2  POINT (115.24424 36.65814)  \n",
       "3  POINT (115.27287 36.65855)  \n",
       "4  POINT (115.30150 36.65897)  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import geopandas as gpd\n",
    "# from shapely.geometry import Point\n",
    "# import pystac_client\n",
    "# import stackstac\n",
    "# import torch\n",
    "# from torchvision import transforms as v2\n",
    "# from box import Box\n",
    "# import yaml\n",
    "# import math\n",
    "# from rasterio.enums import Resampling\n",
    "# from tqdm import tqdm\n",
    "# import rasterio\n",
    "# import warnings\n",
    "# import os\n",
    "# import numpy as np\n",
    "# import rioxarray  # Make sure to import rioxarray to extend xarray\n",
    "\n",
    "# from src.model import ClayMAEModule\n",
    "\n",
    "# # Set the environment variable for requester-pays \n",
    "# # NEED THIS LINE FOR LANDSAT\n",
    "# os.environ['AWS_REQUEST_PAYER'] = 'requester'\n",
    "\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# STAC_API = \"https://earth-search.aws.element84.com/v1\"\n",
    "# COLLECTION = \"landsat-c2-l2\"\n",
    "\n",
    "# # Load the model and metadata\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# ckpt = \"https://clay-model-ckpt.s3.amazonaws.com/v0.5.7/mae_v0.5.7_epoch-13_val-loss-0.3098.ckpt\"\n",
    "# torch.set_default_device(device)\n",
    "\n",
    "# torch.cuda.empty_cache()  # Clear GPU cache\n",
    "\n",
    "# # Assuming grid is a GeoDataFrame with the points\n",
    "# points = grid.to_crs(\"EPSG:4326\").geometry.apply(lambda x: (x.x, x.y)).tolist()\n",
    "\n",
    "# model = ClayMAEModule.load_from_checkpoint(\n",
    "#     ckpt, metadata_path=\"configs/metadata.yaml\", shuffle=False, mask_ratio=0\n",
    "# )\n",
    "# model.eval()\n",
    "# model = model.to(device)\n",
    "\n",
    "# metadata = Box(yaml.safe_load(open(\"configs/metadata.yaml\")))\n",
    "\n",
    "# # Function to normalize timestamp\n",
    "# def normalize_timestamp(date):\n",
    "#     week = date.isocalendar().week * 2 * np.pi / 52\n",
    "#     hour = date.hour * 2 * np.pi / 24\n",
    "#     return (math.sin(week), math.cos(week)), (math.sin(hour), math.cos(hour))\n",
    "\n",
    "# # Function to normalize lat/lon\n",
    "# def normalize_latlon(lat, lon):\n",
    "#     lat = lat * np.pi / 180\n",
    "#     lon = lon * np.pi / 180\n",
    "#     return (math.sin(lat), math.cos(lat)), (math.sin(lon), math.cos(lon))\n",
    "\n",
    "# def to_device(data, device):\n",
    "#     if isinstance(data, torch.Tensor):\n",
    "#         return data.to(device)\n",
    "#     elif isinstance(data, dict):\n",
    "#         return {k: to_device(v, device) for k, v in data.items()}\n",
    "#     elif isinstance(data, list):\n",
    "#         return [to_device(v, device) for v in data]\n",
    "#     return data\n",
    "\n",
    "# def process_point(lon, lat, model, metadata, year, device, j):\n",
    "#     model.to(device)  # Ensure the model is on the correct device\n",
    "#     catalog = pystac_client.Client.open(STAC_API)\n",
    "#     search = catalog.search(\n",
    "#         collections=[COLLECTION],\n",
    "#         datetime=f\"{year}-01-01/{year}-12-31\",\n",
    "#         bbox=(lon - 1e-5, lat - 1e-5, lon + 1e-5, lat + 1e-5),\n",
    "#         max_items=10,\n",
    "#         query={\"eo:cloud_cover\": {\"lt\": 80}},\n",
    "#     )\n",
    "\n",
    "#     all_items = search.get_all_items()\n",
    "#     items = list(all_items)\n",
    "#     if not items:\n",
    "#         return None\n",
    "    \n",
    "#     items = sorted(items, key=lambda x: x.properties.get('eo:cloud_cover', float('inf')))\n",
    "#     lowest_cloud_item = items[0]\n",
    "\n",
    "#     epsg = lowest_cloud_item.properties[\"proj:epsg\"]\n",
    "\n",
    "#     poidf = gpd.GeoDataFrame(\n",
    "#         pd.DataFrame(),\n",
    "#         crs=\"EPSG:4326\",\n",
    "#         geometry=[Point(lon, lat)],\n",
    "#     ).to_crs(epsg)\n",
    "\n",
    "#     coords = poidf.iloc[0].geometry.coords[0]\n",
    "\n",
    "#     size = 256\n",
    "#     gsd = 30\n",
    "#     bounds = (\n",
    "#         coords[0] - (size * gsd) // 2,\n",
    "#         coords[1] - (size * gsd) // 2,\n",
    "#         coords[0] + (size * gsd) // 2,\n",
    "#         coords[1] + (size * gsd) // 2,\n",
    "#     )\n",
    "\n",
    "#     stack = stackstac.stack(\n",
    "#         lowest_cloud_item,\n",
    "#         bounds=bounds,\n",
    "#         snap_bounds=False,\n",
    "#         epsg=epsg,\n",
    "#         resolution=gsd,\n",
    "#         dtype=\"float32\",\n",
    "#         rescale=False,\n",
    "#         fill_value=0,\n",
    "#         assets=[\"blue\", \"green\", \"red\", \"nir\"],\n",
    "#         resampling=Resampling.nearest,\n",
    "#     )\n",
    "\n",
    "#     stack = stack.compute()\n",
    "\n",
    "#     items = []\n",
    "#     dates = []\n",
    "#     for item in all_items:\n",
    "#         if item.datetime.date() not in dates:\n",
    "#             items.append(item)\n",
    "#             dates.append(item.datetime.date())\n",
    "\n",
    "#     platform = \"landsat-c2l1\"\n",
    "#     mean = []\n",
    "#     std = []\n",
    "#     waves = []\n",
    "#     for band in stack.band:\n",
    "#         mean.append(metadata[platform].bands.mean[str(band.values)])\n",
    "#         std.append(metadata[platform].bands.std[str(band.values)])\n",
    "#         waves.append(metadata[platform].bands.wavelength[str(band.values)])\n",
    "\n",
    "#     transform = v2.Compose([v2.Normalize(mean=mean, std=std)])\n",
    "\n",
    "#     datetimes = stack.time.values.astype(\"datetime64[s]\").tolist()\n",
    "#     times = [normalize_timestamp(dat) for dat in datetimes]\n",
    "#     week_norm = [dat[0] for dat in times]\n",
    "#     hour_norm = [dat[1] for dat in times]\n",
    "\n",
    "#     latlons = [normalize_latlon(lat, lon)] * len(times)\n",
    "#     lat_norm = [dat[0] for dat in latlons]\n",
    "#     lon_norm = [dat[1] for dat in latlons]\n",
    "\n",
    "#     pixels = torch.from_numpy(stack.data.astype(np.float32)).to(device)\n",
    "#     pixels = transform(pixels)\n",
    "\n",
    "#     batch_size = 16\n",
    "#     num_batches = math.ceil(len(stack) / batch_size)\n",
    "    \n",
    "#     embeddings_list = []\n",
    "#     for i in range(num_batches):\n",
    "#         start_idx = i * batch_size\n",
    "#         end_idx = min((i + 1) * batch_size, len(stack))\n",
    "        \n",
    "#         batch_pixels = pixels[start_idx:end_idx].to(device)\n",
    "#         batch_time = torch.tensor(np.hstack((week_norm, hour_norm))[start_idx:end_idx], dtype=torch.float32).to(device)\n",
    "#         batch_latlon = torch.tensor(np.hstack((lat_norm, lon_norm))[start_idx:end_idx], dtype=torch.float32).to(device)\n",
    "        \n",
    "#         batch_datacube = {\n",
    "#             \"platform\": platform,\n",
    "#             \"time\": batch_time,\n",
    "#             \"latlon\": batch_latlon,\n",
    "#             \"pixels\": batch_pixels,\n",
    "#             \"gsd\": torch.tensor(stack.gsd.values).to(device),\n",
    "#             \"waves\": torch.tensor(waves).to(device),\n",
    "#         }\n",
    "\n",
    "#         batch_datacube = to_device(batch_datacube, device)\n",
    "\n",
    "#         try:\n",
    "#             model = model.to(device)\n",
    "\n",
    "#             with torch.no_grad():\n",
    "#                 unmsk_patch, _, _, _ = model.model.encoder(batch_datacube)\n",
    "#             batch_embeddings = unmsk_patch[:, 0, :].cpu().numpy()\n",
    "#             embeddings_list.append(batch_embeddings)\n",
    "#         except RuntimeError as e:\n",
    "#             if \"out of memory\" in str(e):\n",
    "#                 print(f\"GPU OOM for point ({lon}, {lat}), batch {i+1}/{num_batches}. Trying CPU...\")\n",
    "#                 device = torch.device(\"cpu\")\n",
    "#                 batch_datacube = to_device(batch_datacube, device)\n",
    "#                 model = model.to(device)\n",
    "#                 with torch.no_grad():\n",
    "#                     unmsk_patch, _, _, _ = model.model.encoder(batch_datacube)\n",
    "#                 batch_embeddings = unmsk_patch[:, 0, :].numpy()\n",
    "#                 embeddings_list.append(batch_embeddings)\n",
    "#                 device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#             else:\n",
    "#                 raise e\n",
    "\n",
    "#     embeddings = np.concatenate(embeddings_list, axis=0)\n",
    "#     return embeddings\n",
    "\n",
    "# # Specify the years for the datetime range in the search\n",
    "# year = 2015\n",
    "\n",
    "# # Initialize an empty dictionary to store results for both years\n",
    "# results_dict = {\"lon\": [], \"lat\": [], \"embeddings\": []}\n",
    "\n",
    "# # Iterate through the points and process each one for both years\n",
    "# for i, point in enumerate(tqdm(points)):\n",
    "#     lon, lat = point\n",
    "#     results_dict[\"lon\"].append(lon)\n",
    "#     results_dict[\"lat\"].append(lat)\n",
    "    \n",
    "#     embeddings = process_point(lon, lat, model, metadata, year, device, i)\n",
    "#     if embeddings is not None:\n",
    "#         results_dict[\"embeddings\"].append(embeddings)\n",
    "#     else:\n",
    "#         results_dict[\"embeddings\"].append(None)\n",
    "\n",
    "# # Create a DataFrame from the results\n",
    "# df = pd.DataFrame(results_dict)\n",
    "\n",
    "# # Convert to a GeoDataFrame\n",
    "# gdf_results = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.lon, df.lat))\n",
    "\n",
    "# # Output the resulting GeoDataFrame\n",
    "# gdf_results.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gdf_copy = gdf_results.copy()\n",
    "# gdf_copy[\"embeddings\"] = [embedding.flatten() if embedding is not None and embedding.size > 0 else None for embedding in gdf_results[\"embeddings\"]]\n",
    "\n",
    "# gdf_copy.to_parquet(\"train_data/challenge_4.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Join grain/maize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Raster 1: 100%|██████████| 3864/3864 [00:41<00:00, 93.87it/s]\n",
      "Processing Raster 2: 100%|██████████| 3864/3864 [00:41<00:00, 92.87it/s]\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import rioxarray\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# Load your GeoDataFrame\n",
    "gdf = gpd.read_parquet(\"train_data/challenge_4.parquet\")\n",
    "\n",
    "# Load the TIFF files as xarray Datasets\n",
    "raster1_path = 'train_data/MaizeYield2015.tif'\n",
    "raster2_path = 'train_data/WheatYield2015.tif'\n",
    "\n",
    "# Convert the TIFFs to xarray datasets\n",
    "raster1_xr = rioxarray.open_rasterio(raster1_path)\n",
    "raster2_xr = rioxarray.open_rasterio(raster2_path)\n",
    "raster1_xr = raster1_xr.rio.reproject(4326)\n",
    "raster2_xr = raster2_xr.rio.reproject(4326)\n",
    "\n",
    "# Create the geometry column using Point\n",
    "gdf['geometry'] = gdf.apply(lambda row: Point(row['lon'], row['lat']), axis=1)\n",
    "\n",
    "# Function to get raster value at geometry\n",
    "def get_raster_value_at_geometry_xr(geometry, raster_xr):\n",
    "    try:\n",
    "        clipped = raster_xr.rio.clip([geometry], raster_xr.rio.crs)\n",
    "        return float(clipped.sum().values)\n",
    "    except rioxarray.exceptions.NoDataInBounds as e:\n",
    "        return np.nan\n",
    "\n",
    "def process_row(geom, raster_xr):\n",
    "    return get_raster_value_at_geometry_xr(geom, raster_xr)\n",
    "\n",
    "# Parallel processing with joblib\n",
    "n_jobs = 12 # Use all available cores\n",
    "\n",
    "# Use tqdm with joblib for progress bar\n",
    "raster_values_1 = Parallel(n_jobs=n_jobs)(\n",
    "    delayed(process_row)(geom, raster1_xr) for geom in tqdm(gdf['geometry'], desc=\"Processing Raster 1\"))\n",
    "\n",
    "raster_values_2 = Parallel(n_jobs=n_jobs)(\n",
    "    delayed(process_row)(geom, raster2_xr) for geom in tqdm(gdf['geometry'], desc=\"Processing Raster 2\"))\n",
    "\n",
    "# Assign the values back to the GeoDataFrame\n",
    "gdf['maize'] = raster_values_1\n",
    "gdf['wheat'] = raster_values_2\n",
    "\n",
    "# Set values greater than 1 to NaN for raster_value_1\n",
    "gdf.loc[np.abs(gdf['maize']) > 1e37, 'maize'] = np.nan\n",
    "\n",
    "# Set values greater than 1 to NaN for raster_value_2\n",
    "gdf.loc[np.abs(gdf['wheat']) > 1e37, 'wheat'] = np.nan\n",
    "\n",
    "# Save the result to a parquet file\n",
    "gdf.to_parquet(\"train_data/challenge_4.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1723904598.498452    4063 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "[I 2024-08-17 14:23:18,521] A new study created in memory with name: no-name-d23c7732-6d64-4e6c-95bd-9c77daf1b7fd\n",
      "I0000 00:00:1723904598.517595    4063 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1723904598.519046    4063 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1723904598.533579    4063 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1723904598.535224    4063 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1723904598.536638    4063 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1723904598.544079    4063 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1723904598.545546    4063 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1723904598.547102    4063 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-17 14:23:18.549684: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 17057 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1723904599.667561   13952 service.cc:146] XLA service 0x7fd9f400dc80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1723904599.667599   13952 service.cc:154]   StreamExecutor device (0): NVIDIA A10G, Compute Capability 8.6\n",
      "2024-08-17 14:23:19.769696: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-08-17 14:23:20.003010: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m51/78\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 13395885.0000 - mae: 2847.1567 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1723904601.482028   13952 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 66ms/step - loss: 12479227.0000 - mae: 2854.7195 - val_loss: 8959275.0000 - val_mae: 2790.1565\n",
      "Epoch 2/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8985395.0000 - mae: 2756.6404 - val_loss: 8718582.0000 - val_mae: 2782.7007\n",
      "Epoch 3/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8884582.0000 - mae: 2762.8616 - val_loss: 8935061.0000 - val_mae: 2798.8865\n",
      "Epoch 4/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8794687.0000 - mae: 2730.0493 - val_loss: 8450502.0000 - val_mae: 2699.9514\n",
      "Epoch 5/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8739431.0000 - mae: 2720.1646 - val_loss: 8833516.0000 - val_mae: 2747.8289\n",
      "Epoch 6/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8469648.0000 - mae: 2636.0183 - val_loss: 8511874.0000 - val_mae: 2663.2949\n",
      "Epoch 7/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8491243.0000 - mae: 2649.1245 - val_loss: 8321384.0000 - val_mae: 2665.7336\n",
      "Epoch 8/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8547454.0000 - mae: 2656.4302 - val_loss: 8474343.0000 - val_mae: 2677.2754\n",
      "Epoch 9/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8201888.5000 - mae: 2596.3845 - val_loss: 8351861.5000 - val_mae: 2663.8428\n",
      "Epoch 10/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8499637.0000 - mae: 2625.6223 - val_loss: 8357561.5000 - val_mae: 2659.6736\n",
      "Epoch 11/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8366383.5000 - mae: 2625.9595 - val_loss: 8382738.0000 - val_mae: 2680.1057\n",
      "Epoch 12/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8381332.5000 - mae: 2630.8386 - val_loss: 8299625.0000 - val_mae: 2690.3264\n",
      "Epoch 13/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8272399.0000 - mae: 2604.0693 - val_loss: 8387992.5000 - val_mae: 2657.9033\n",
      "Epoch 14/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8422493.0000 - mae: 2619.5676 - val_loss: 8264551.0000 - val_mae: 2672.9265\n",
      "Epoch 15/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8455468.0000 - mae: 2634.7222 - val_loss: 8540410.0000 - val_mae: 2643.2202\n",
      "Epoch 16/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8213810.5000 - mae: 2540.0076 - val_loss: 8328160.5000 - val_mae: 2659.9792\n",
      "Epoch 17/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8188849.5000 - mae: 2584.7166 - val_loss: 8236792.0000 - val_mae: 2647.0254\n",
      "Epoch 18/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8291790.5000 - mae: 2599.2031 - val_loss: 8256101.5000 - val_mae: 2656.0376\n",
      "Epoch 19/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8141891.5000 - mae: 2585.0232 - val_loss: 8198729.5000 - val_mae: 2651.3289\n",
      "Epoch 20/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8325550.5000 - mae: 2604.6711 - val_loss: 8135562.5000 - val_mae: 2631.8967\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-17 14:23:31,545] Trial 0 finished with value: 2837.744908981487 and parameters: {'n_layers': 2, 'n_units_l0': 463, 'n_units_l1': 77, 'dropout_rate': 0.3893498596969507, 'learning_rate': 0.005693862022830421}. Best is trial 0 with value: 2837.744908981487.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 55ms/step - loss: 11911663.0000 - mae: 2798.2166 - val_loss: 8973060.0000 - val_mae: 2820.6611\n",
      "Epoch 2/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8657893.0000 - mae: 2754.3335 - val_loss: 8688951.0000 - val_mae: 2754.0042\n",
      "Epoch 3/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8597334.0000 - mae: 2713.1086 - val_loss: 8627245.0000 - val_mae: 2699.7588\n",
      "Epoch 4/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8394706.0000 - mae: 2645.9146 - val_loss: 8453811.0000 - val_mae: 2699.8235\n",
      "Epoch 5/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8396845.0000 - mae: 2675.6377 - val_loss: 8453883.0000 - val_mae: 2718.5002\n",
      "Epoch 6/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8549446.0000 - mae: 2686.3560 - val_loss: 8423916.0000 - val_mae: 2695.1763\n",
      "Epoch 7/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8322585.0000 - mae: 2633.0295 - val_loss: 8361358.5000 - val_mae: 2661.7097\n",
      "Epoch 8/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8413470.0000 - mae: 2655.4695 - val_loss: 8610595.0000 - val_mae: 2649.9373\n",
      "Epoch 9/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8276123.5000 - mae: 2580.4111 - val_loss: 8283644.5000 - val_mae: 2635.8838\n",
      "Epoch 10/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8017210.0000 - mae: 2543.9651 - val_loss: 8677751.0000 - val_mae: 2674.2065\n",
      "Epoch 11/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8217829.0000 - mae: 2608.0825 - val_loss: 8245397.5000 - val_mae: 2659.3330\n",
      "Epoch 12/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8338054.0000 - mae: 2616.2917 - val_loss: 8227165.0000 - val_mae: 2627.6091\n",
      "Epoch 13/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8063440.0000 - mae: 2575.5261 - val_loss: 8339323.0000 - val_mae: 2634.4153\n",
      "Epoch 14/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 7986857.0000 - mae: 2555.6033 - val_loss: 8518564.0000 - val_mae: 2660.5708\n",
      "Epoch 15/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8072027.5000 - mae: 2572.5820 - val_loss: 8144506.5000 - val_mae: 2633.2495\n",
      "Epoch 16/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 7964626.0000 - mae: 2542.1519 - val_loss: 8274571.0000 - val_mae: 2645.3579\n",
      "Epoch 17/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8032781.5000 - mae: 2558.1982 - val_loss: 8707165.0000 - val_mae: 2648.9568\n",
      "Epoch 18/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8008888.0000 - mae: 2528.1326 - val_loss: 8329293.0000 - val_mae: 2626.1855\n",
      "Epoch 19/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8077285.0000 - mae: 2554.0984 - val_loss: 8226543.0000 - val_mae: 2610.6248\n",
      "Epoch 20/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 7848847.5000 - mae: 2505.8474 - val_loss: 8185075.0000 - val_mae: 2628.6443\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-17 14:23:42,671] Trial 1 finished with value: 2839.2108198504184 and parameters: {'n_layers': 2, 'n_units_l0': 360, 'n_units_l1': 98, 'dropout_rate': 0.23835991060603529, 'learning_rate': 0.006465715329743283}. Best is trial 0 with value: 2837.744908981487.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 73ms/step - loss: 14281127.0000 - mae: 2865.1799 - val_loss: 8974042.0000 - val_mae: 2814.5981\n",
      "Epoch 2/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9033265.0000 - mae: 2788.3345 - val_loss: 8668347.0000 - val_mae: 2773.0928\n",
      "Epoch 3/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8620760.0000 - mae: 2752.6399 - val_loss: 8602095.0000 - val_mae: 2766.4580\n",
      "Epoch 4/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8476016.0000 - mae: 2711.9226 - val_loss: 8578802.0000 - val_mae: 2757.6665\n",
      "Epoch 5/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8633542.0000 - mae: 2740.5283 - val_loss: 8571723.0000 - val_mae: 2683.2249\n",
      "Epoch 6/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8272938.0000 - mae: 2627.1116 - val_loss: 8574506.0000 - val_mae: 2723.0911\n",
      "Epoch 7/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8217936.5000 - mae: 2636.9949 - val_loss: 8534005.0000 - val_mae: 2679.2900\n",
      "Epoch 8/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8361967.5000 - mae: 2636.8118 - val_loss: 8623834.0000 - val_mae: 2680.0620\n",
      "Epoch 9/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8162526.5000 - mae: 2617.4949 - val_loss: 8293439.5000 - val_mae: 2656.9795\n",
      "Epoch 10/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8176641.0000 - mae: 2619.0188 - val_loss: 8242657.5000 - val_mae: 2680.9573\n",
      "Epoch 11/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8138364.5000 - mae: 2601.1978 - val_loss: 8336892.0000 - val_mae: 2674.9819\n",
      "Epoch 12/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8192617.5000 - mae: 2614.0061 - val_loss: 8311154.5000 - val_mae: 2667.4861\n",
      "Epoch 13/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8169708.0000 - mae: 2590.1602 - val_loss: 8435619.0000 - val_mae: 2643.9827\n",
      "Epoch 14/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8058297.5000 - mae: 2560.6877 - val_loss: 8295390.0000 - val_mae: 2621.6467\n",
      "Epoch 15/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8035180.5000 - mae: 2557.8186 - val_loss: 8337730.5000 - val_mae: 2667.1233\n",
      "Epoch 16/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8115871.5000 - mae: 2578.5403 - val_loss: 8265176.5000 - val_mae: 2630.9504\n",
      "Epoch 17/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7962865.5000 - mae: 2562.2820 - val_loss: 8259412.0000 - val_mae: 2626.9878\n",
      "Epoch 18/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7954916.5000 - mae: 2546.9707 - val_loss: 8372177.0000 - val_mae: 2632.1245\n",
      "Epoch 19/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8033465.0000 - mae: 2543.8110 - val_loss: 8189757.0000 - val_mae: 2624.5571\n",
      "Epoch 20/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7956192.5000 - mae: 2563.3452 - val_loss: 8395617.0000 - val_mae: 2599.9734\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 79ms/step \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-17 14:23:56,718] Trial 2 finished with value: 2869.2926498074635 and parameters: {'n_layers': 3, 'n_units_l0': 414, 'n_units_l1': 90, 'n_units_l2': 303, 'dropout_rate': 0.25607141108645187, 'learning_rate': 0.001445054415016444}. Best is trial 0 with value: 2837.744908981487.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 33ms/step - loss: 17384506.0000 - mae: 2958.4805 - val_loss: 14996435.0000 - val_mae: 2933.0276\n",
      "Epoch 2/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 13435386.0000 - mae: 2885.2771 - val_loss: 10352660.0000 - val_mae: 2879.0132\n",
      "Epoch 3/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 9654779.0000 - mae: 2804.0972 - val_loss: 9333461.0000 - val_mae: 2842.1912\n",
      "Epoch 4/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 9038369.0000 - mae: 2805.4436 - val_loss: 9035123.0000 - val_mae: 2832.5898\n",
      "Epoch 5/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8854935.0000 - mae: 2806.3601 - val_loss: 8918725.0000 - val_mae: 2823.3733\n",
      "Epoch 6/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8658586.0000 - mae: 2779.3345 - val_loss: 8854133.0000 - val_mae: 2817.2698\n",
      "Epoch 7/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8729461.0000 - mae: 2791.6101 - val_loss: 8801689.0000 - val_mae: 2811.8213\n",
      "Epoch 8/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8708959.0000 - mae: 2794.9744 - val_loss: 8759290.0000 - val_mae: 2801.5352\n",
      "Epoch 9/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8694772.0000 - mae: 2779.1792 - val_loss: 8744730.0000 - val_mae: 2793.1648\n",
      "Epoch 10/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8612885.0000 - mae: 2762.4919 - val_loss: 8687384.0000 - val_mae: 2791.6548\n",
      "Epoch 11/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8460428.0000 - mae: 2740.6997 - val_loss: 8648480.0000 - val_mae: 2780.7041\n",
      "Epoch 12/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8241745.5000 - mae: 2713.6287 - val_loss: 8609543.0000 - val_mae: 2771.0549\n",
      "Epoch 13/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8458014.0000 - mae: 2737.3718 - val_loss: 8566922.0000 - val_mae: 2765.7122\n",
      "Epoch 14/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8337150.0000 - mae: 2714.0144 - val_loss: 8557594.0000 - val_mae: 2759.4756\n",
      "Epoch 15/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8555612.0000 - mae: 2745.1838 - val_loss: 8549990.0000 - val_mae: 2753.7292\n",
      "Epoch 16/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8494033.0000 - mae: 2722.1877 - val_loss: 8520141.0000 - val_mae: 2746.5933\n",
      "Epoch 17/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8318872.0000 - mae: 2686.6863 - val_loss: 8492813.0000 - val_mae: 2741.6074\n",
      "Epoch 18/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8388940.0000 - mae: 2707.6731 - val_loss: 8462503.0000 - val_mae: 2731.8020\n",
      "Epoch 19/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8458843.0000 - mae: 2718.2837 - val_loss: 8460262.0000 - val_mae: 2727.5217\n",
      "Epoch 20/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8284956.0000 - mae: 2678.4109 - val_loss: 8423255.0000 - val_mae: 2719.7397\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-17 14:24:04,100] Trial 3 finished with value: 2877.8748344019145 and parameters: {'n_layers': 1, 'n_units_l0': 201, 'dropout_rate': 0.4409541006564382, 'learning_rate': 0.0016921887566440665}. Best is trial 0 with value: 2837.744908981487.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 46ms/step - loss: 14767067.0000 - mae: 2885.5044 - val_loss: 8981394.0000 - val_mae: 2823.0242\n",
      "Epoch 2/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8905823.0000 - mae: 2776.3018 - val_loss: 8773018.0000 - val_mae: 2802.1782\n",
      "Epoch 3/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8691644.0000 - mae: 2736.0481 - val_loss: 8627619.0000 - val_mae: 2788.2649\n",
      "Epoch 4/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8521707.0000 - mae: 2717.3708 - val_loss: 8583093.0000 - val_mae: 2757.5369\n",
      "Epoch 5/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8443622.0000 - mae: 2695.4302 - val_loss: 8542317.0000 - val_mae: 2730.2083\n",
      "Epoch 6/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8561479.0000 - mae: 2677.5017 - val_loss: 8548053.0000 - val_mae: 2720.4265\n",
      "Epoch 7/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8535877.0000 - mae: 2673.1191 - val_loss: 8517037.0000 - val_mae: 2705.0232\n",
      "Epoch 8/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8534289.0000 - mae: 2668.0483 - val_loss: 8369645.0000 - val_mae: 2705.5713\n",
      "Epoch 9/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8403934.0000 - mae: 2643.1401 - val_loss: 8417295.0000 - val_mae: 2710.9988\n",
      "Epoch 10/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8396231.0000 - mae: 2651.7219 - val_loss: 8482498.0000 - val_mae: 2678.1462\n",
      "Epoch 11/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8381615.0000 - mae: 2630.7644 - val_loss: 8268529.5000 - val_mae: 2675.9973\n",
      "Epoch 12/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8383230.5000 - mae: 2635.9658 - val_loss: 8612986.0000 - val_mae: 2699.1804\n",
      "Epoch 13/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8240543.0000 - mae: 2624.2227 - val_loss: 8363110.5000 - val_mae: 2676.9121\n",
      "Epoch 14/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8107364.0000 - mae: 2588.1782 - val_loss: 8360377.5000 - val_mae: 2687.1152\n",
      "Epoch 15/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8164393.0000 - mae: 2594.9209 - val_loss: 8364710.5000 - val_mae: 2676.4224\n",
      "Epoch 16/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8025391.5000 - mae: 2566.7722 - val_loss: 8410520.0000 - val_mae: 2665.6672\n",
      "Epoch 17/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8209198.5000 - mae: 2590.5044 - val_loss: 8285294.0000 - val_mae: 2664.5601\n",
      "Epoch 18/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8073146.5000 - mae: 2572.9175 - val_loss: 8215253.5000 - val_mae: 2641.0186\n",
      "Epoch 19/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8277363.0000 - mae: 2593.7642 - val_loss: 8295966.0000 - val_mae: 2661.9360\n",
      "Epoch 20/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8139566.5000 - mae: 2562.8284 - val_loss: 8259542.5000 - val_mae: 2667.2261\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-17 14:24:14,076] Trial 4 finished with value: 2852.5199810506183 and parameters: {'n_layers': 2, 'n_units_l0': 198, 'n_units_l1': 85, 'dropout_rate': 0.4108308923556877, 'learning_rate': 0.0033137192398227165}. Best is trial 0 with value: 2837.744908981487.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_layers': 2, 'n_units_l0': 463, 'n_units_l1': 77, 'dropout_rate': 0.3893498596969507, 'learning_rate': 0.005693862022830421}\n",
      "Epoch 1/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 29ms/step - loss: 12516891.0000 - mae: 2865.1230 - val_loss: 9052540.0000 - val_mae: 2825.2517\n",
      "Epoch 2/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8771631.0000 - mae: 2763.2871 - val_loss: 8896587.0000 - val_mae: 2752.4192\n",
      "Epoch 3/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8817680.0000 - mae: 2740.2012 - val_loss: 8732529.0000 - val_mae: 2744.5759\n",
      "Epoch 4/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8530377.0000 - mae: 2676.2192 - val_loss: 8417034.0000 - val_mae: 2699.5869\n",
      "Epoch 5/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8604024.0000 - mae: 2697.1816 - val_loss: 8566418.0000 - val_mae: 2695.3264\n",
      "Epoch 6/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8401383.0000 - mae: 2628.4185 - val_loss: 8428404.0000 - val_mae: 2718.2075\n",
      "Epoch 7/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8612652.0000 - mae: 2685.4456 - val_loss: 8364700.5000 - val_mae: 2662.6104\n",
      "Epoch 8/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8293080.5000 - mae: 2616.7124 - val_loss: 8586336.0000 - val_mae: 2679.8733\n",
      "Epoch 9/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8439582.0000 - mae: 2629.6472 - val_loss: 8383661.5000 - val_mae: 2675.1240\n",
      "Epoch 10/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8414658.0000 - mae: 2631.7417 - val_loss: 8325294.5000 - val_mae: 2703.7678\n",
      "Epoch 11/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8109369.0000 - mae: 2574.3003 - val_loss: 8239258.5000 - val_mae: 2670.2366\n",
      "Epoch 12/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8490109.0000 - mae: 2640.2783 - val_loss: 8373650.0000 - val_mae: 2683.5520\n",
      "Epoch 13/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8253888.5000 - mae: 2603.6697 - val_loss: 8313230.0000 - val_mae: 2678.4285\n",
      "Epoch 14/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8225835.5000 - mae: 2595.2163 - val_loss: 8390055.0000 - val_mae: 2674.9292\n",
      "Epoch 15/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8227837.0000 - mae: 2586.7129 - val_loss: 8168752.5000 - val_mae: 2641.0283\n",
      "Epoch 16/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8122962.0000 - mae: 2554.7585 - val_loss: 8246656.5000 - val_mae: 2648.8997\n",
      "Epoch 17/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8273482.0000 - mae: 2616.5527 - val_loss: 8453750.0000 - val_mae: 2685.8079\n",
      "Epoch 18/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 7978350.0000 - mae: 2564.7319 - val_loss: 8320309.5000 - val_mae: 2671.1406\n",
      "Epoch 19/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8006552.5000 - mae: 2562.6885 - val_loss: 8157586.0000 - val_mae: 2649.3523\n",
      "Epoch 20/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 7991258.5000 - mae: 2554.9680 - val_loss: 8205340.0000 - val_mae: 2634.7961\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 8368090.0000 - mae: 2623.8223\n",
      "Test MAE: 2590.4355\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: [3099.1521812  2584.47239963]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['models/4_scaler.joblib']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import optuna\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Input\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_shape, layers, dropout_rate, learning_rate, device):\n",
    "        self.input_shape = input_shape\n",
    "        self.layers = layers\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.device = device\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Input(shape=(self.input_shape,)))\n",
    "        for layer_size in self.layers:\n",
    "            model.add(Dense(layer_size, activation='relu'))\n",
    "            model.add(Dropout(self.dropout_rate))\n",
    "        model.add(Dense(2, activation='linear'))  # Output layer for predicting 'maize' and 'wheat'\n",
    "        return model\n",
    "\n",
    "    def compile_model(self):\n",
    "        optimizer = Adam(learning_rate=self.learning_rate)\n",
    "        self.model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mae'])  # Use MSE for regression\n",
    "\n",
    "    def train_model(self, X_train, y_train, epochs=20, batch_size=32, validation_split=0.2):\n",
    "        with tf.device(self.device):\n",
    "            history = self.model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=validation_split)\n",
    "        return history\n",
    "\n",
    "    def evaluate_model(self, X_test, y_test):\n",
    "        with tf.device(self.device):\n",
    "            loss, mae = self.model.evaluate(X_test, y_test)\n",
    "        return loss, mae\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        with tf.device(self.device):\n",
    "            predictions = self.model.predict(X_test)\n",
    "        return predictions\n",
    "\n",
    "    def calculate_rmse(self, y_test, predictions):\n",
    "        return np.sqrt(mean_squared_error(y_test, predictions, multioutput='raw_values'))\n",
    "    \n",
    "    def save_model(self, filename):\n",
    "        self.model.save(filename)\n",
    "\n",
    "    @classmethod\n",
    "    def load_model(cls, filename, input_shape, device):\n",
    "        loaded_model = tf.keras.models.load_model(filename)\n",
    "        nn = cls(input_shape, [], 0, 0, device)  # Dummy values for layers, dropout_rate, and learning_rate\n",
    "        nn.model = loaded_model\n",
    "        return nn\n",
    "    \n",
    "def objective(trial):\n",
    "    layers = []\n",
    "    for i in range(trial.suggest_int('n_layers', 1, 3)):\n",
    "        layers.append(trial.suggest_int(f'n_units_l{i}', 64, 512))\n",
    "    \n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.2, 0.5)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2)\n",
    "    \n",
    "    nn = NeuralNetwork(input_shape=X_train.shape[1], device=device, layers=layers, dropout_rate=dropout_rate, learning_rate=learning_rate)\n",
    "    nn.compile_model()\n",
    "    \n",
    "    nn.train_model(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2)\n",
    "    \n",
    "    predictions = nn.predict(X_test)\n",
    "    rmse = nn.calculate_rmse(y_test, predictions)\n",
    "    \n",
    "    return np.mean(rmse)\n",
    "\n",
    "# Assuming combined_gdf is already loaded\n",
    "combined_gdf = gdf.copy()\n",
    "combined_gdf_filtered = combined_gdf.dropna(subset=['embeddings'])\n",
    "\n",
    "# Set NaN values in 'maize' and 'wheat' to 0\n",
    "combined_gdf_filtered['maize'] = combined_gdf_filtered['maize'].fillna(0)\n",
    "combined_gdf_filtered['wheat'] = combined_gdf_filtered['wheat'].fillna(0)\n",
    "\n",
    "# Flatten the nested arrays\n",
    "def flatten_embeddings(embedding):\n",
    "    if isinstance(embedding, np.ndarray) and embedding.ndim > 0:\n",
    "        return np.concatenate(embedding) if embedding.ndim > 1 else embedding.flatten()\n",
    "    else:\n",
    "        return np.array([])  # Handle cases where embedding might be zero-dimensional or empty\n",
    "\n",
    "X = np.array([flatten_embeddings(x) for x in combined_gdf_filtered['embeddings'] if flatten_embeddings(x).size > 0])\n",
    "y = combined_gdf_filtered[['maize', 'wheat']].to_numpy()\n",
    "\n",
    "# Ensure that X and y have matching lengths after filtering\n",
    "X = np.array([x for i, x in enumerate(X) if flatten_embeddings(combined_gdf_filtered['embeddings'].iloc[i]).size > 0])\n",
    "y = y[:X.shape[0]]  # Adjust y to have the same number of samples as X\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "device = '/GPU:0' if tf.config.list_physical_devices('GPU') else '/CPU:0'\n",
    "\n",
    "# Optimize the hyperparameters\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=5)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(study.best_params)\n",
    "\n",
    "# Example usage with the best hyperparameters\n",
    "best_params = study.best_params\n",
    "layers = [best_params[f'n_units_l{i}'] for i in range(best_params['n_layers'])]\n",
    "dropout_rate = best_params['dropout_rate']\n",
    "learning_rate = best_params['learning_rate']\n",
    "\n",
    "nn = NeuralNetwork(input_shape=X_train.shape[1], device=device, layers=layers, dropout_rate=dropout_rate, learning_rate=learning_rate)\n",
    "nn.compile_model()\n",
    "history = nn.train_model(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2)\n",
    "loss, mae = nn.evaluate_model(X_test, y_test)\n",
    "print(f'Test MAE: {mae:.4f}')\n",
    "predictions = nn.predict(X_test)\n",
    "rmse = nn.calculate_rmse(y_test, predictions)\n",
    "print(f'Test RMSE: {rmse}')\n",
    "\n",
    "# Save the model\n",
    "nn.save_model('models/task_4_model.h5')\n",
    "\n",
    "# Save scaler\n",
    "joblib.dump(scaler, 'models/4_scaler.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>POLYGON ((108.53924 34.58379, 108.53924 34.278...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            geometry\n",
       "0  POLYGON ((108.53924 34.58379, 108.53924 34.278..."
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "# Load the GeoJSON file\n",
    "geojson_path = 'test_data/challenge_4_bb.geojson'\n",
    "gdf = gpd.read_file(geojson_path)\n",
    "gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>POLYGON ((274295.683 3829641.202, 273472.551 3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            geometry\n",
       "0  POLYGON ((274295.683 3829641.202, 273472.551 3..."
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyproj\n",
    "\n",
    "def get_utm_zone(longitude):\n",
    "    return int((longitude + 180) / 6) + 1\n",
    "\n",
    "# Get the bounds of the geometry\n",
    "minx, miny, maxx, maxy = gdf.geometry.bounds.iloc[0]\n",
    "\n",
    "# Calculate UTM zone\n",
    "utm_zone = get_utm_zone(minx)\n",
    "\n",
    "# Check for a suitable projection using pyproj\n",
    "proj = pyproj.Proj(proj='utm', zone=utm_zone, ellps='WGS84')\n",
    "\n",
    "# Get the corresponding EPSG code for the UTM zone using pyproj\n",
    "utm_crs = pyproj.CRS(f\"+proj=utm +zone={utm_zone} +datum=WGS84\")\n",
    "epsg_code = utm_crs.to_epsg()\n",
    "\n",
    "# Reproject the GeoDataFrame to the chosen EPSG code\n",
    "gdf = gdf.to_crs(epsg=epsg_code)\n",
    "gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>POINT (273472.551 3794871.701)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>POINT (276032.551 3794871.701)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>POINT (278592.551 3794871.701)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>POINT (281152.551 3794871.701)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>POINT (283712.551 3794871.701)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>POINT (304192.551 3828151.701)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>POINT (306752.551 3828151.701)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>POINT (309312.551 3828151.701)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>POINT (311872.551 3828151.701)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>POINT (314432.551 3828151.701)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>238 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           geometry\n",
       "0    POINT (273472.551 3794871.701)\n",
       "1    POINT (276032.551 3794871.701)\n",
       "2    POINT (278592.551 3794871.701)\n",
       "3    POINT (281152.551 3794871.701)\n",
       "4    POINT (283712.551 3794871.701)\n",
       "..                              ...\n",
       "233  POINT (304192.551 3828151.701)\n",
       "234  POINT (306752.551 3828151.701)\n",
       "235  POINT (309312.551 3828151.701)\n",
       "236  POINT (311872.551 3828151.701)\n",
       "237  POINT (314432.551 3828151.701)\n",
       "\n",
       "[238 rows x 1 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create a grid of points 5120m apart\n",
    "x = np.arange(gdf.total_bounds[0], gdf.total_bounds[2], 2560)\n",
    "y = np.arange(gdf.total_bounds[1], gdf.total_bounds[3], 2560)\n",
    "xx, yy = np.meshgrid(x, y)\n",
    "points = np.vstack([xx.ravel(), yy.ravel()]).T\n",
    "\n",
    "grid = gpd.GeoDataFrame(geometry=gpd.points_from_xy(points[:, 0], points[:, 1], crs=gdf.crs))\n",
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [03:24<00:00,  1.16it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>embeddings</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>108.539475</td>\n",
       "      <td>34.270343</td>\n",
       "      <td>[[0.040251017, -0.09942115, -0.05468018, 0.221...</td>\n",
       "      <td>POINT (108.53948 34.27034)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>108.567260</td>\n",
       "      <td>34.270898</td>\n",
       "      <td>[[0.044807635, -0.11502899, -0.046713702, 0.22...</td>\n",
       "      <td>POINT (108.56726 34.27090)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>108.595045</td>\n",
       "      <td>34.271447</td>\n",
       "      <td>[[0.07913431, -0.104119375, -0.06128583, 0.237...</td>\n",
       "      <td>POINT (108.59504 34.27145)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>108.622831</td>\n",
       "      <td>34.271989</td>\n",
       "      <td>[[0.10027202, -0.097676344, -0.09782456, 0.220...</td>\n",
       "      <td>POINT (108.62283 34.27199)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>108.650617</td>\n",
       "      <td>34.272525</td>\n",
       "      <td>[[0.1512495, -0.06380439, -0.10349344, 0.18129...</td>\n",
       "      <td>POINT (108.65062 34.27253)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          lon        lat                                         embeddings  \\\n",
       "0  108.539475  34.270343  [[0.040251017, -0.09942115, -0.05468018, 0.221...   \n",
       "1  108.567260  34.270898  [[0.044807635, -0.11502899, -0.046713702, 0.22...   \n",
       "2  108.595045  34.271447  [[0.07913431, -0.104119375, -0.06128583, 0.237...   \n",
       "3  108.622831  34.271989  [[0.10027202, -0.097676344, -0.09782456, 0.220...   \n",
       "4  108.650617  34.272525  [[0.1512495, -0.06380439, -0.10349344, 0.18129...   \n",
       "\n",
       "                     geometry  \n",
       "0  POINT (108.53948 34.27034)  \n",
       "1  POINT (108.56726 34.27090)  \n",
       "2  POINT (108.59504 34.27145)  \n",
       "3  POINT (108.62283 34.27199)  \n",
       "4  POINT (108.65062 34.27253)  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import pystac_client\n",
    "import stackstac\n",
    "import torch\n",
    "from torchvision import transforms as v2\n",
    "from box import Box\n",
    "import yaml\n",
    "import math\n",
    "from rasterio.enums import Resampling\n",
    "from tqdm import tqdm\n",
    "import rasterio\n",
    "import warnings\n",
    "import os\n",
    "import numpy as np\n",
    "import rioxarray  # Make sure to import rioxarray to extend xarray\n",
    "\n",
    "from src.model import ClayMAEModule\n",
    "\n",
    "# Set the environment variable for requester-pays \n",
    "# NEED THIS LINE FOR LANDSAT\n",
    "os.environ['AWS_REQUEST_PAYER'] = 'requester'\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "STAC_API = \"https://earth-search.aws.element84.com/v1\"\n",
    "COLLECTION = \"landsat-c2-l2\"\n",
    "\n",
    "# Load the model and metadata\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ckpt = \"https://clay-model-ckpt.s3.amazonaws.com/v0.5.7/mae_v0.5.7_epoch-13_val-loss-0.3098.ckpt\"\n",
    "torch.set_default_device(device)\n",
    "\n",
    "torch.cuda.empty_cache()  # Clear GPU cache\n",
    "\n",
    "# Assuming grid is a GeoDataFrame with the points\n",
    "points = grid.to_crs(\"EPSG:4326\").geometry.apply(lambda x: (x.x, x.y)).tolist()\n",
    "\n",
    "model = ClayMAEModule.load_from_checkpoint(\n",
    "    ckpt, metadata_path=\"configs/metadata.yaml\", shuffle=False, mask_ratio=0\n",
    ")\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "\n",
    "metadata = Box(yaml.safe_load(open(\"configs/metadata.yaml\")))\n",
    "\n",
    "# Function to normalize timestamp\n",
    "def normalize_timestamp(date):\n",
    "    week = date.isocalendar().week * 2 * np.pi / 52\n",
    "    hour = date.hour * 2 * np.pi / 24\n",
    "    return (math.sin(week), math.cos(week)), (math.sin(hour), math.cos(hour))\n",
    "\n",
    "# Function to normalize lat/lon\n",
    "def normalize_latlon(lat, lon):\n",
    "    lat = lat * np.pi / 180\n",
    "    lon = lon * np.pi / 180\n",
    "    return (math.sin(lat), math.cos(lat)), (math.sin(lon), math.cos(lon))\n",
    "\n",
    "def to_device(data, device):\n",
    "    if isinstance(data, torch.Tensor):\n",
    "        return data.to(device)\n",
    "    elif isinstance(data, dict):\n",
    "        return {k: to_device(v, device) for k, v in data.items()}\n",
    "    elif isinstance(data, list):\n",
    "        return [to_device(v, device) for v in data]\n",
    "    return data\n",
    "\n",
    "def process_point(lon, lat, model, metadata, year, device, j):\n",
    "    model.to(device)  # Ensure the model is on the correct device\n",
    "    catalog = pystac_client.Client.open(STAC_API)\n",
    "    search = catalog.search(\n",
    "        collections=[COLLECTION],\n",
    "        datetime=f\"{year}-01-01/{year}-12-31\",\n",
    "        bbox=(lon - 1e-5, lat - 1e-5, lon + 1e-5, lat + 1e-5),\n",
    "        max_items=10,\n",
    "        query={\"eo:cloud_cover\": {\"lt\": 80}},\n",
    "    )\n",
    "\n",
    "    all_items = search.get_all_items()\n",
    "    items = list(all_items)\n",
    "    if not items:\n",
    "        return None\n",
    "    \n",
    "    items = sorted(items, key=lambda x: x.properties.get('eo:cloud_cover', float('inf')))\n",
    "    lowest_cloud_item = items[0]\n",
    "\n",
    "    epsg = lowest_cloud_item.properties[\"proj:epsg\"]\n",
    "\n",
    "    poidf = gpd.GeoDataFrame(\n",
    "        pd.DataFrame(),\n",
    "        crs=\"EPSG:4326\",\n",
    "        geometry=[Point(lon, lat)],\n",
    "    ).to_crs(epsg)\n",
    "\n",
    "    coords = poidf.iloc[0].geometry.coords[0]\n",
    "\n",
    "    size = 256\n",
    "    gsd = 30\n",
    "    bounds = (\n",
    "        coords[0] - (size * gsd) // 2,\n",
    "        coords[1] - (size * gsd) // 2,\n",
    "        coords[0] + (size * gsd) // 2,\n",
    "        coords[1] + (size * gsd) // 2,\n",
    "    )\n",
    "\n",
    "    stack = stackstac.stack(\n",
    "        lowest_cloud_item,\n",
    "        bounds=bounds,\n",
    "        snap_bounds=False,\n",
    "        epsg=epsg,\n",
    "        resolution=gsd,\n",
    "        dtype=\"float32\",\n",
    "        rescale=False,\n",
    "        fill_value=0,\n",
    "        assets=[\"blue\", \"green\", \"red\", \"nir\"],\n",
    "        resampling=Resampling.nearest,\n",
    "    )\n",
    "\n",
    "    stack = stack.compute()\n",
    "\n",
    "    items = []\n",
    "    dates = []\n",
    "    for item in all_items:\n",
    "        if item.datetime.date() not in dates:\n",
    "            items.append(item)\n",
    "            dates.append(item.datetime.date())\n",
    "\n",
    "    platform = \"landsat-c2l1\"\n",
    "    mean = []\n",
    "    std = []\n",
    "    waves = []\n",
    "    for band in stack.band:\n",
    "        mean.append(metadata[platform].bands.mean[str(band.values)])\n",
    "        std.append(metadata[platform].bands.std[str(band.values)])\n",
    "        waves.append(metadata[platform].bands.wavelength[str(band.values)])\n",
    "\n",
    "    transform = v2.Compose([v2.Normalize(mean=mean, std=std)])\n",
    "\n",
    "    datetimes = stack.time.values.astype(\"datetime64[s]\").tolist()\n",
    "    times = [normalize_timestamp(dat) for dat in datetimes]\n",
    "    week_norm = [dat[0] for dat in times]\n",
    "    hour_norm = [dat[1] for dat in times]\n",
    "\n",
    "    latlons = [normalize_latlon(lat, lon)] * len(times)\n",
    "    lat_norm = [dat[0] for dat in latlons]\n",
    "    lon_norm = [dat[1] for dat in latlons]\n",
    "\n",
    "    pixels = torch.from_numpy(stack.data.astype(np.float32)).to(device)\n",
    "    pixels = transform(pixels)\n",
    "\n",
    "    batch_size = 16\n",
    "    num_batches = math.ceil(len(stack) / batch_size)\n",
    "    \n",
    "    embeddings_list = []\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, len(stack))\n",
    "        \n",
    "        batch_pixels = pixels[start_idx:end_idx].to(device)\n",
    "        batch_time = torch.tensor(np.hstack((week_norm, hour_norm))[start_idx:end_idx], dtype=torch.float32).to(device)\n",
    "        batch_latlon = torch.tensor(np.hstack((lat_norm, lon_norm))[start_idx:end_idx], dtype=torch.float32).to(device)\n",
    "        \n",
    "        batch_datacube = {\n",
    "            \"platform\": platform,\n",
    "            \"time\": batch_time,\n",
    "            \"latlon\": batch_latlon,\n",
    "            \"pixels\": batch_pixels,\n",
    "            \"gsd\": torch.tensor(stack.gsd.values).to(device),\n",
    "            \"waves\": torch.tensor(waves).to(device),\n",
    "        }\n",
    "\n",
    "        batch_datacube = to_device(batch_datacube, device)\n",
    "\n",
    "        try:\n",
    "            model = model.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                unmsk_patch, _, _, _ = model.model.encoder(batch_datacube)\n",
    "            batch_embeddings = unmsk_patch[:, 0, :].cpu().numpy()\n",
    "            embeddings_list.append(batch_embeddings)\n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e):\n",
    "                print(f\"GPU OOM for point ({lon}, {lat}), batch {i+1}/{num_batches}. Trying CPU...\")\n",
    "                device = torch.device(\"cpu\")\n",
    "                batch_datacube = to_device(batch_datacube, device)\n",
    "                model = model.to(device)\n",
    "                with torch.no_grad():\n",
    "                    unmsk_patch, _, _, _ = model.model.encoder(batch_datacube)\n",
    "                batch_embeddings = unmsk_patch[:, 0, :].numpy()\n",
    "                embeddings_list.append(batch_embeddings)\n",
    "                device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "    embeddings = np.concatenate(embeddings_list, axis=0)\n",
    "    return embeddings\n",
    "\n",
    "# Specify the years for the datetime range in the search\n",
    "year = 2013\n",
    "\n",
    "# Initialize an empty dictionary to store results for both years\n",
    "results_dict = {\"lon\": [], \"lat\": [], \"embeddings\": []}\n",
    "\n",
    "# Iterate through the points and process each one for both years\n",
    "for i, point in enumerate(tqdm(points)):\n",
    "    lon, lat = point\n",
    "    results_dict[\"lon\"].append(lon)\n",
    "    results_dict[\"lat\"].append(lat)\n",
    "    \n",
    "    embeddings = process_point(lon, lat, model, metadata, year, device, i)\n",
    "    if embeddings is not None:\n",
    "        results_dict[\"embeddings\"].append(embeddings)\n",
    "    else:\n",
    "        results_dict[\"embeddings\"].append(None)\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "df = pd.DataFrame(results_dict)\n",
    "\n",
    "# Convert to a GeoDataFrame\n",
    "gdf_results = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.lon, df.lat))\n",
    "\n",
    "# Output the resulting GeoDataFrame\n",
    "gdf_results.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_copy = gdf_results.copy()\n",
    "gdf_copy[\"embeddings\"] = [embedding.flatten() if embedding is not None and embedding.size > 0 else None for embedding in gdf_results[\"embeddings\"]]\n",
    "\n",
    "gdf_copy.to_parquet(\"test_data/challenge_4.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import optuna\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Input\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_shape, layers, dropout_rate, learning_rate, device):\n",
    "        self.input_shape = input_shape\n",
    "        self.layers = layers\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.device = device\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Input(shape=(self.input_shape,)))\n",
    "        for layer_size in self.layers:\n",
    "            model.add(Dense(layer_size, activation='relu'))\n",
    "            model.add(Dropout(self.dropout_rate))\n",
    "        model.add(Dense(2, activation='linear'))  # Output layer for predicting 'maize' and 'wheat'\n",
    "        return model\n",
    "\n",
    "    def compile_model(self):\n",
    "        optimizer = Adam(learning_rate=self.learning_rate)\n",
    "        self.model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mae'])  # Use MSE for regression\n",
    "\n",
    "    def train_model(self, X_train, y_train, epochs=20, batch_size=32, validation_split=0.2):\n",
    "        with tf.device(self.device):\n",
    "            history = self.model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=validation_split)\n",
    "        return history\n",
    "\n",
    "    def evaluate_model(self, X_test, y_test):\n",
    "        with tf.device(self.device):\n",
    "            loss, mae = self.model.evaluate(X_test, y_test)\n",
    "        return loss, mae\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        with tf.device(self.device):\n",
    "            predictions = self.model.predict(X_test)\n",
    "        return predictions\n",
    "\n",
    "    def calculate_rmse(self, y_test, predictions):\n",
    "        return np.sqrt(mean_squared_error(y_test, predictions, multioutput='raw_values'))\n",
    "    \n",
    "    def save_model(self, filename):\n",
    "        self.model.save(filename)\n",
    "\n",
    "    @classmethod\n",
    "    def load_model(cls, filename, input_shape, device):\n",
    "        loaded_model = tf.keras.models.load_model(filename)\n",
    "        nn = cls(input_shape, [], 0, 0, device)  # Dummy values for layers, dropout_rate, and learning_rate\n",
    "        nn.model = loaded_model\n",
    "        return nn\n",
    "    \n",
    "def objective(trial):\n",
    "    layers = []\n",
    "    for i in range(trial.suggest_int('n_layers', 1, 3)):\n",
    "        layers.append(trial.suggest_int(f'n_units_l{i}', 64, 512))\n",
    "    \n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.2, 0.5)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2)\n",
    "    \n",
    "    nn = NeuralNetwork(input_shape=X_train.shape[1], device=device, layers=layers, dropout_rate=dropout_rate, learning_rate=learning_rate)\n",
    "    nn.compile_model()\n",
    "    \n",
    "    nn.train_model(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2)\n",
    "    \n",
    "    predictions = nn.predict(X_test)\n",
    "    rmse = nn.calculate_rmse(y_test, predictions)\n",
    "    \n",
    "    return np.mean(rmse)\n",
    "\n",
    "# Assuming combined_gdf is already loaded\n",
    "combined_gdf = gdf_copy.copy()\n",
    "combined_gdf_filtered = combined_gdf.dropna(subset=['embeddings'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 240ms/step\n"
     ]
    }
   ],
   "source": [
    "# Detect if GPU is available\n",
    "device = '/GPU:0' if tf.config.list_physical_devices('GPU') else '/CPU:0'\n",
    "\n",
    "# Load the model\n",
    "loaded_nn = NeuralNetwork.load_model('models/task_4_model.h5', input_shape=768, device=device)\n",
    "\n",
    "# Prepare your new data (assuming it's in the same format as your training data)\n",
    "new_data = np.squeeze(combined_gdf_filtered['embeddings'].tolist())\n",
    "new_data = pd.DataFrame(new_data)  # Ensure the new data is in DataFrame format\n",
    "\n",
    "# Make predictions\n",
    "new_predictions = loaded_nn.predict(new_data)\n",
    "\n",
    "gdf_copy[['pred_maize','pred_wheat']] = new_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Raster 1: 100%|██████████| 238/238 [00:02<00:00, 114.18it/s]\n",
      "Processing Raster 2: 100%|██████████| 238/238 [00:02<00:00, 98.71it/s]\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import rioxarray\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# Load the TIFF files as xarray Datasets\n",
    "raster1_path = 'test_data/MaizeYield2013.tif'\n",
    "raster2_path = 'test_data/WheatYield2013.tif'\n",
    "\n",
    "# Convert the TIFFs to xarray datasets\n",
    "raster1_xr = rioxarray.open_rasterio(raster1_path)\n",
    "raster2_xr = rioxarray.open_rasterio(raster2_path)\n",
    "raster1_xr = raster1_xr.rio.reproject(4326)\n",
    "raster2_xr = raster2_xr.rio.reproject(4326)\n",
    "\n",
    "# Create the geometry column using Point\n",
    "gdf_copy['geometry'] = gdf_copy.apply(lambda row: Point(row['lon'], row['lat']), axis=1)\n",
    "\n",
    "# Function to get raster value at geometry\n",
    "def get_raster_value_at_geometry_xr(geometry, raster_xr):\n",
    "    try:\n",
    "        clipped = raster_xr.rio.clip([geometry], raster_xr.rio.crs)\n",
    "        return float(clipped.sum().values)\n",
    "    except rioxarray.exceptions.NoDataInBounds as e:\n",
    "        return np.nan\n",
    "\n",
    "def process_row(geom, raster_xr):\n",
    "    return get_raster_value_at_geometry_xr(geom, raster_xr)\n",
    "\n",
    "# Parallel processing with joblib\n",
    "n_jobs = 12 # Use all available cores\n",
    "\n",
    "# Use tqdm with joblib for progress bar\n",
    "raster_values_1 = Parallel(n_jobs=n_jobs)(\n",
    "    delayed(process_row)(geom, raster1_xr) for geom in tqdm(gdf_copy['geometry'], desc=\"Processing Raster 1\"))\n",
    "\n",
    "raster_values_2 = Parallel(n_jobs=n_jobs)(\n",
    "    delayed(process_row)(geom, raster2_xr) for geom in tqdm(gdf_copy['geometry'], desc=\"Processing Raster 2\"))\n",
    "\n",
    "# Assign the values back to the GeoDataFrame\n",
    "gdf_copy['maize'] = raster_values_1\n",
    "gdf_copy['wheat'] = raster_values_2\n",
    "\n",
    "# Set values greater than 1 to NaN for raster_value_1\n",
    "gdf_copy.loc[np.abs(gdf_copy['maize']) > 1e37, 'maize'] = np.nan\n",
    "\n",
    "# Set values greater than 1 to NaN for raster_value_2\n",
    "gdf_copy.loc[np.abs(gdf_copy['wheat']) > 1e37, 'wheat'] = np.nan\n",
    "\n",
    "gdf_copy['maize'] = gdf_copy['maize'].fillna(0)\n",
    "gdf_copy['wheat'] = gdf_copy['wheat'].fillna(0)\n",
    "\n",
    "# Save the result to a parquet file\n",
    "gdf_copy.to_parquet(\"test_data/challenge_4.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set RMSE: 3235.1679227391887\n",
      "Test set corr: -0.12314035777962042\n"
     ]
    }
   ],
   "source": [
    "print(\"Test set RMSE:\", np.sqrt(np.mean((gdf_copy['pred_maize']-gdf_copy['maize'])**2)))\n",
    "print(\"Test set corr:\", np.corrcoef(gdf_copy['pred_maize'],gdf_copy['maize'])[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_maize</th>\n",
       "      <th>pred_wheat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000.960449</td>\n",
       "      <td>975.279114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>996.647217</td>\n",
       "      <td>969.454590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>969.085144</td>\n",
       "      <td>943.091736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>817.738770</td>\n",
       "      <td>796.544983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>550.917603</td>\n",
       "      <td>542.767334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>331.973877</td>\n",
       "      <td>320.300781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>473.824341</td>\n",
       "      <td>455.954010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>650.904785</td>\n",
       "      <td>626.890137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>823.732727</td>\n",
       "      <td>789.586365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>893.729431</td>\n",
       "      <td>860.341309</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>238 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      pred_maize  pred_wheat\n",
       "0    1000.960449  975.279114\n",
       "1     996.647217  969.454590\n",
       "2     969.085144  943.091736\n",
       "3     817.738770  796.544983\n",
       "4     550.917603  542.767334\n",
       "..           ...         ...\n",
       "233   331.973877  320.300781\n",
       "234   473.824341  455.954010\n",
       "235   650.904785  626.890137\n",
       "236   823.732727  789.586365\n",
       "237   893.729431  860.341309\n",
       "\n",
       "[238 rows x 2 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdf_copy[['pred_maize','pred_wheat']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "claymodel-latest-v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
